{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4503297e",
   "metadata": {},
   "source": [
    "## 0. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "859a7e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Initialize mediapipe pose class\n",
    "mp_pose = mp.solutions.pose\n",
    "pose_landmark = mp_pose.PoseLandmark\n",
    "landmarks_to_draw = [landmark for landmark in mp_pose.PoseLandmark]\n",
    "posture_data = {}  # Dictionary to store posture analysis results\n",
    "buffer, sml_buffer = 10, 3  # Buffers for thresholding angle differences in joint analysis\n",
    "sign = \"| \"  # Separator used in string formatting for posture conditions\n",
    "choice = None\n",
    "\n",
    "def initialize_capture():\n",
    "    \"\"\"\n",
    "    Initializes video capture from a file or a camera based on user input.\n",
    "\n",
    "    Returns:\n",
    "    tuple: (cv2.VideoCapture object, boolean indicating if the source is an image, source path or None)\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            input_choice = int(input(\"Enter 1 for image/video file, 2 for camera: \"))\n",
    "            if input_choice == 1:\n",
    "                while True:  # Loop to keep asking for the file path until a valid one is provided\n",
    "                    path = input(\"Enter the path of the image or video file: \")\n",
    "                    if os.path.exists(path):\n",
    "                        cap = cv2.VideoCapture(path)\n",
    "                        if cap.isOpened() or path.endswith(('jpg', 'jpeg', 'png')):  # Check if it's an image or video can be opened\n",
    "                            return cap, path.endswith(('jpg', 'jpeg', 'png')), path\n",
    "                        else:\n",
    "                            print(\"Failed to open the file. Please make sure it is a valid image or video file.\")\n",
    "                    else:\n",
    "                        print(\"The file does not exist. Please enter a valid path.\")\n",
    "            elif input_choice == 2:\n",
    "                return cv2.VideoCapture(int(input(\"Camera index (usually 0): \"))), False, None\n",
    "            else:\n",
    "                print(\"Invalid choice. Please enter 1 for image/video file or 2 for camera.\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a number (1 or 2).\")\n",
    "\n",
    "def setup_output_writer(cap, is_image):\n",
    "    \"\"\"\n",
    "    Sets up a writer for saving the output, depending on whether the input is an image or a video.\n",
    "\n",
    "    Args:\n",
    "    cap: The VideoCapture object.\n",
    "    is_image (bool): Indicates whether the input source is an image.\n",
    "\n",
    "    Returns:\n",
    "    Either a cv2.VideoWriter object for videos or a string path for images.\n",
    "    \"\"\"\n",
    "    if is_image:\n",
    "        out_path = input(\"Enter the path to save the processed image (include file extension): \")\n",
    "        return out_path\n",
    "    else:\n",
    "        out_path = input(\"Path for output video (.avi or .mp4): \")\n",
    "        frame_rate = cap.get(cv2.CAP_PROP_FPS)  # Get the frame rate of the input video\n",
    "        dimensions = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "\n",
    "        # Selecting the codec based on file extension\n",
    "        if out_path.endswith('.mp4'):\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'MP4V')  # Codec for .mp4 files\n",
    "        else:\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Default codec for .avi files\n",
    "\n",
    "        return cv2.VideoWriter(out_path, fourcc, frame_rate, dimensions)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def process_frame(frame, pose, anal_func, detect_func):\n",
    "    \"\"\"\n",
    "    Processes a single frame for pose detection and applies analysis and detection functions.\n",
    "\n",
    "    Args:\n",
    "    frame: The current frame from the video source.\n",
    "    pose: The Mediapipe pose object for detecting poses.\n",
    "    anal_func (function): Function to apply pose analysis.\n",
    "    detect_func (function): Function to annotate detected poses on the frame.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The processed frame with pose annotations.\n",
    "    \"\"\"\n",
    "    # Flip the frame horizontally\n",
    "    if choice == 1:\n",
    "        frame = cv2.flip(frame, 1)\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image)\n",
    "    if results.pose_landmarks:\n",
    "        # Convert image back to BGR and apply detection function\n",
    "        return detect_func(cv2.cvtColor(image, cv2.COLOR_RGB2BGR), results, *anal_func(results.pose_landmarks.landmark, mp_pose))\n",
    "    return cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "def get_point(landmarks, landmark):\n",
    "    \"\"\"\n",
    "    Extracts the x and y coordinates of a specified landmark from the landmarks.\n",
    "\n",
    "    Args:\n",
    "    landmarks (list): A list of Mediapipe pose landmarks.\n",
    "    landmark (mp_pose.PoseLandmark): The specific landmark to extract.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Coordinates (x, y) of the specified landmark.\n",
    "    \"\"\"\n",
    "    return landmarks[landmark.value].x, landmarks[landmark.value].y\n",
    "\n",
    "def calculate_angle(a, b, c, reverse=False):\n",
    "    \"\"\"\n",
    "    Calculates the angle formed by three points, with an option to reverse the direction.\n",
    "\n",
    "    Args:\n",
    "    a, b, c (tuple): Coordinates of the points forming the angle (each as an (x, y) tuple).\n",
    "    reverse (bool): If True, calculates the angle in the reverse direction.\n",
    "\n",
    "    Returns:\n",
    "    float: The calculated angle in degrees.\n",
    "    \"\"\"\n",
    "    a, b, c = np.array(a), np.array(b), np.array(c)\n",
    "\n",
    "    # Calculating the angle depending on the reverse flag\n",
    "    if reverse:\n",
    "        radians = np.arctan2(b[1] - a[1], b[0] - a[0]) - np.arctan2(c[1] - a[1], c[0] - a[0])\n",
    "    else:\n",
    "        radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(a[1] - b[1], a[0] - b[0])\n",
    "\n",
    "    angle = np.degrees(radians)\n",
    "\n",
    "    # Adjusting angle to the range [0, 360] and then normalizing to [-180, 180]\n",
    "    angle = (angle + 360) % 360\n",
    "    if angle > 180:\n",
    "        angle -= 360\n",
    "\n",
    "    return angle\n",
    "\n",
    "def draw_colored_connection(image, results, start_idx, end_idx, color=(255, 0, 0), thickness=2):\n",
    "    \"\"\"\n",
    "    Draws a colored line between two landmarks on the image.\n",
    "\n",
    "    Args:\n",
    "    image: The frame on which to draw the line.\n",
    "    results: Mediapipe pose detection results.\n",
    "    start_idx, end_idx (int): Indices of the start and end landmarks.\n",
    "    color (tuple): The color of the line to draw.\n",
    "    \"\"\"\n",
    "    if results.pose_landmarks:\n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "        start, end = landmarks[start_idx], landmarks[end_idx]\n",
    "        cv2.line(image, (int(start.x * image.shape[1]), int(start.y * image.shape[0])), \n",
    "                 (int(end.x * image.shape[1]), int(end.y * image.shape[0])), color, thickness)\n",
    "\n",
    "def calculate_posture_ratio():\n",
    "    \"\"\"\n",
    "    Calculates the ratio of different postures detected in terms of their percentage.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary with the percentage of each posture condition.\n",
    "    \"\"\"\n",
    "    section_percentages = {}\n",
    "    top_level_percentages = {}\n",
    "\n",
    "    # Calculate the total count of postures (both incorrect and correct)\n",
    "    total_postures = sum(sum(postures.values()) if isinstance(postures, dict) else postures \n",
    "                         for section, postures in posture_data.items())\n",
    "\n",
    "    # Calculate the total correct count\n",
    "    correct_data = posture_data.get(\"Correct\", {})\n",
    "    correct_count = sum(correct_data.values()) if isinstance(correct_data, dict) else correct_data\n",
    "\n",
    "    # Calculate percentages for individual conditions within each section\n",
    "    for section, postures in posture_data.items():\n",
    "        if section == \"Correct\":\n",
    "            continue  # Skip the \"Correct\" section for this part\n",
    "\n",
    "        section_total = sum(postures.values()) if isinstance(postures, dict) else postures\n",
    "        section_percentages[section] = {condition: f\"{(count / section_total) * 100:.1f}%\" \n",
    "                                        for condition, count in postures.items()} if isinstance(postures, dict) else {}\n",
    "\n",
    "    # Calculate overall issue percentages\n",
    "    for section, postures in posture_data.items():\n",
    "        if section == \"Correct\":\n",
    "            continue  # Skip the \"Correct\" section for this part\n",
    "\n",
    "        section_count = sum(postures.values()) if isinstance(postures, dict) else postures\n",
    "        top_level_percentages[f\"{section} Issue\"] = f\"{(section_count / total_postures) * 100:.1f}%\"\n",
    "\n",
    "    # Calculate the correct ratio\n",
    "    correct_ratio = (correct_count / (total_postures+0.0000001)) * 100\n",
    "    top_level_percentages[\"Correct Ratio\"] = f\"{correct_ratio:.1f}%\"\n",
    "\n",
    "    return {**section_percentages, **top_level_percentages}\n",
    "\n",
    "def update_posture_data(section, posture_conditions, detected_postures):\n",
    "    \"\"\"\n",
    "    Updates the posture data dictionary based on the detected postures.\n",
    "\n",
    "    Args:\n",
    "    section (str): The section/category of the posture (e.g., \"Knee\", \"Hip\").\n",
    "    posture_conditions (list): List of possible conditions within the section.\n",
    "    detected_postures (str): The postures detected in the current frame.\n",
    "    \"\"\"\n",
    "    if section == \"Correct\":\n",
    "        posture_data[section] = {\"Correct\": posture_data.get(section, {}).get(\"Correct\", 0) + 1}\n",
    "    else:\n",
    "        detected_postures_list = [p.strip() for p in detected_postures.split(\" and \") if p.strip() in posture_conditions]\n",
    "        for posture in detected_postures_list:\n",
    "            posture_data.setdefault(section, {}).setdefault(posture, 0)\n",
    "            posture_data[section][posture] += 1\n",
    "            \n",
    "def draw_labeled_box(image, results, joint_landmarks, joint_names, angles, font_scale=0.3, font_thickness=1,\n",
    "                     box_color=(255, 255, 255), text_color=(139, 0, 0), edge_color=(230, 216, 173)):\n",
    "    \"\"\"\n",
    "    For each joint, calculates the position, draws a labeled text box with angle information on the image.\n",
    "\n",
    "    Args:\n",
    "    - image (numpy.ndarray): The image on which to draw.\n",
    "    - results (object): The detected pose landmarks from a pose estimation model.\n",
    "    - joint_landmarks (list): The list of landmark enums to draw labels for.\n",
    "    - joint_names (list): The list of joint names to draw labels for.\n",
    "    - angles (tuple): Tuple containing angle values for each joint.\n",
    "    - font_scale, font_thickness (int): Font properties.\n",
    "    - box_color, text_color, edge_color (tuple): Box, text, and edge colors.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The image with labeled boxes drawn on it.\n",
    "    \"\"\"\n",
    "    for joint_index, angle in enumerate(angles):\n",
    "        joint_landmark = results.pose_landmarks.landmark[joint_landmarks[joint_index]]\n",
    "        angle_text = f\"{joint_names[joint_index]}: {round(angle, 1)}\"\n",
    "        text_x = int(joint_landmark.x * image.shape[1]) + 10\n",
    "        text_y = int(joint_landmark.y * image.shape[0]) - 10\n",
    "\n",
    "        # Determine the size of the text box\n",
    "        text_size, _ = cv2.getTextSize(angle_text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness)\n",
    "        text_width, text_height = text_size\n",
    "\n",
    "        box_start = (text_x - 2, text_y + 2)\n",
    "        box_end = (text_x + text_width + 2, text_y - text_height - 2)\n",
    "\n",
    "        # Draw the filled rectangle (background)\n",
    "        cv2.rectangle(image, box_start, box_end, box_color, cv2.FILLED)\n",
    "        # Draw the border rectangle (edges)\n",
    "        cv2.rectangle(image, box_start, box_end, edge_color, 1)\n",
    "\n",
    "        # Now put the text (in specified text color)\n",
    "        text_org = (text_x, text_y)\n",
    "        cv2.putText(image, angle_text, text_org, cv2.FONT_HERSHEY_SIMPLEX, font_scale, text_color, font_thickness, cv2.LINE_AA)\n",
    "\n",
    "    return image\n",
    "\n",
    "def run_estimation(anal_func, detect_func):\n",
    "    \"\"\"\n",
    "    Main function to run the posture estimation process.\n",
    "\n",
    "    Args:\n",
    "    anal_func (function): Function that performs the analysis of postures.\n",
    "    detect_func (function): Function that detects and annotates poses on the frame.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame containing the percentage ratios of posture conditions.\n",
    "    \"\"\"\n",
    "    cap, is_image, input_path = initialize_capture()\n",
    "    if not cap:\n",
    "        return\n",
    "\n",
    "    out = setup_output_writer(cap, is_image)\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.95, min_tracking_confidence=0.95) as pose:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "\n",
    "            processed_frame = process_frame(frame, pose, anal_func, detect_func)\n",
    "            cv2.imshow('Mediapipe Feed', processed_frame)\n",
    "\n",
    "            if not is_image and out:\n",
    "                out.write(processed_frame)  # Save to video if out is a VideoWriter object\n",
    "            elif is_image:\n",
    "                cv2.imwrite(out, processed_frame)  # Save to image file if out is a string\n",
    "                break  # No need to process further frames for an image\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q') or is_image: break\n",
    "\n",
    "        cap.release()\n",
    "        if not is_image and out:\n",
    "            out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    posture_percentages = calculate_posture_ratio()\n",
    "    \n",
    "    \n",
    "    # Flatten the nested dictionaries\n",
    "    data = []\n",
    "    for section, conditions in posture_percentages.items():\n",
    "        if isinstance(conditions, dict):\n",
    "            for condition, percentage in conditions.items():\n",
    "                data.append({'Section': section, 'Condition': condition, 'Percentage': percentage})\n",
    "        else:\n",
    "            data.append({'Section': section, 'Condition': '', 'Percentage': conditions})\n",
    "    df = pd.DataFrame(data)\n",
    "    posture_data.clear()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8f6f4c",
   "metadata": {},
   "source": [
    "## 1.1 Squat Front"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707b7fd9",
   "metadata": {},
   "source": [
    "### 1.1.1 Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb2da94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squat_front_analysis(landmarks, mp_pose):\n",
    "    \"\"\"\n",
    "    Analyzes front-view squat posture based on landmarks.\n",
    "\n",
    "    Args:\n",
    "    - landmarks (list): List of detected pose landmarks.\n",
    "    - mp_pose (module): Mediapipe pose module for landmark references.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Returns a tuple containing the detected posture and a tuple of angles (left knee angle, right knee angle, hip angle difference).\n",
    "    \"\"\"\n",
    "\n",
    "    # Get points for hips, knees, and FOOT_INDEX\n",
    "    left_hip, left_knee, left_foot_index = map(lambda lm: get_point(landmarks, lm), [pose_landmark.LEFT_HIP, pose_landmark.LEFT_KNEE, pose_landmark.LEFT_FOOT_INDEX])\n",
    "    right_hip, right_knee, right_foot_index = map(lambda lm: get_point(landmarks, lm), [pose_landmark.RIGHT_HIP, pose_landmark.RIGHT_KNEE, pose_landmark.RIGHT_FOOT_INDEX])\n",
    "    \n",
    "    # Calculate knee angles\n",
    "    left_knee_angle = calculate_angle(left_hip, left_knee, left_foot_index)\n",
    "    right_knee_angle = calculate_angle(right_hip, right_knee, right_foot_index)\n",
    "    \n",
    "    # Calcuate hip angles\n",
    "    left_hip_angle = abs(calculate_angle(right_hip, left_hip, [left_hip[0], 0]))\n",
    "    right_hip_angle = abs(calculate_angle(left_hip, right_hip, [right_hip[0], 0]))\n",
    "    \n",
    "    # Determine if hips are level\n",
    "    left_hip_level = left_hip_angle - right_hip_angle > sml_buffer\n",
    "    right_hip_level = right_hip_angle - left_hip_angle > sml_buffer\n",
    "    \n",
    "    \n",
    "    correct_degree = 140\n",
    "    \n",
    "    # Outward position logic\n",
    "    left_knee_outward = (left_knee_angle + correct_degree) > buffer\n",
    "    right_knee_outward = (right_knee_angle - correct_degree) < -buffer\n",
    "\n",
    "    # Inward position logic\n",
    "    left_knee_inward = (left_knee_angle + correct_degree) < -buffer\n",
    "    right_knee_inward = (right_knee_angle - correct_degree) > buffer\n",
    "\n",
    "    \n",
    "    # update posture condition\n",
    "    knee_postures = [\"Knees outwards\", \"Knees inwards\"]\n",
    "    hip_postures = [\"H.L. hip\", \"H.R. hip\"]\n",
    "\n",
    "    posture = \"\"\n",
    "    if (left_knee_outward or right_knee_outward) and (abs(left_knee_angle) < correct_degree + buffer and abs(right_knee_angle) < correct_degree + buffer):\n",
    "        posture += sign + knee_postures[0]\n",
    "    elif (left_knee_inward or right_knee_inward) and (abs(left_knee_angle) < correct_degree + buffer and abs(right_knee_angle) < correct_degree + buffer):\n",
    "        posture += sign + knee_postures[1]\n",
    "    if left_hip_level:\n",
    "        posture += sign + hip_postures[0]\n",
    "    elif right_hip_level:\n",
    "        posture += sign + hip_postures[1]\n",
    "    if posture == \"\":\n",
    "        posture += \"| Correct\"\n",
    "\n",
    "    detected_postures = posture.split(sign)\n",
    "    for detected_posture in detected_postures:\n",
    "        detected_posture = detected_posture.strip()\n",
    "        if detected_posture in knee_postures:\n",
    "            update_posture_data(\"Knee\", knee_postures, detected_posture)\n",
    "        elif detected_posture in hip_postures:\n",
    "            update_posture_data(\"Hip\", hip_postures, detected_posture)\n",
    "        elif detected_posture == \"Correct\":\n",
    "            posture_data[\"Correct\"] = posture_data.get(\"Correct\", 0) + 1\n",
    "\n",
    "    # Return posture and angles as a tuple\n",
    "    return posture, (left_knee_angle, right_knee_angle, (left_hip_angle - right_hip_angle))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5431f3",
   "metadata": {},
   "source": [
    "### 1.1.2 Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7147ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squat_front_detection(image, results, knee_position, angles):\n",
    "    \"\"\"\n",
    "    Analyzes and annotates a front-view image of a squatting posture.\n",
    "\n",
    "    Args:\n",
    "    - image (numpy.ndarray): The image on which to perform the analysis and annotations.\n",
    "    - results (object): The detected pose landmarks from a pose estimation model.\n",
    "    - knee_position (str): Description of the knee position (e.g., \"Correct\", \"Too forward\").\n",
    "    - angles (tuple): A tuple containing the angles (left knee angle, right knee angle, hip angle difference).\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The annotated image with landmarks, angles, and posture information.\n",
    "    \"\"\"\n",
    "    # Unpack angles to get the knee angles and hip angle difference\n",
    "    left_knee_angle, right_knee_angle, hip_angle_diff = angles\n",
    "    if results.pose_landmarks:\n",
    "        # Specify the landmarks to draw\n",
    "        landmarks_to_draw = [\n",
    "            mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.LEFT_KNEE, mp_pose.PoseLandmark.LEFT_FOOT_INDEX,\n",
    "            mp_pose.PoseLandmark.RIGHT_HIP, mp_pose.PoseLandmark.RIGHT_KNEE, mp_pose.PoseLandmark.RIGHT_FOOT_INDEX\n",
    "        ]\n",
    "\n",
    "        # Draw the specified landmarks\n",
    "        for landmark in landmarks_to_draw:\n",
    "            landmark_point = results.pose_landmarks.landmark[landmark]\n",
    "            cv2.circle(image, (int(landmark_point.x * image.shape[1]), int(landmark_point.y * image.shape[0])), 5, (0, 255, 0), -1)\n",
    "\n",
    "        # Draw the lines from hip to knee to toe for both legs\n",
    "        draw_colored_connection(image, results, mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.LEFT_KNEE)\n",
    "        draw_colored_connection(image, results, mp_pose.PoseLandmark.LEFT_KNEE, mp_pose.PoseLandmark.LEFT_FOOT_INDEX)\n",
    "        draw_colored_connection(image, results, mp_pose.PoseLandmark.RIGHT_HIP, mp_pose.PoseLandmark.RIGHT_KNEE)\n",
    "        draw_colored_connection(image, results, mp_pose.PoseLandmark.RIGHT_KNEE, mp_pose.PoseLandmark.RIGHT_FOOT_INDEX)\n",
    "        \n",
    "        # Draw the line connecting both hips\n",
    "        draw_colored_connection(image, results, mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.RIGHT_HIP)\n",
    "\n",
    "        # Display angles near the knees\n",
    "        for idx, angle in enumerate((left_knee_angle, right_knee_angle)):\n",
    "            knee_landmark = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_KNEE if idx == 0 else mp_pose.PoseLandmark.RIGHT_KNEE]\n",
    "            cv2.putText(image, f\"{'Left' if idx == 0 else 'Right'} Knee: {abs(round(angle, 1))}\", \n",
    "                        (int(knee_landmark.x * image.shape[1]), int(knee_landmark.y * image.shape[0] - 10)), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "            \n",
    "        # Display the hip angle difference near the hip landmark\n",
    "        hip_landmark = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP]\n",
    "        cv2.putText(image, f'Hip Diff: {round(hip_angle_diff, 1)}', \n",
    "                (int(hip_landmark.x * image.shape[1]), int(hip_landmark.y * image.shape[0] - 20)), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "        # Add text for knee position if necessary\n",
    "        cv2.putText(image, f'Posture: {knee_position}', (10, 30), cv2.FONT_HERSHEY_COMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0df6385",
   "metadata": {},
   "source": [
    "## 1.2 Squat Side"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26487618",
   "metadata": {},
   "source": [
    "### 1.2.1 Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f8dbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squat_side_analysis(landmarks, mp_pose):\n",
    "    \"\"\"\n",
    "    Analyzes front-view squat posture based on landmarks.\n",
    "\n",
    "    Args:\n",
    "    - landmarks (list): List of detected pose landmarks.\n",
    "    - mp_pose (module): Mediapipe pose module for landmark references.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Returns a tuple containing the detected posture and a tuple of angles (left knee angle, right knee angle, hip angle difference).\n",
    "    \"\"\"\n",
    "    # Get points for shoulders, hips, knees, and ankles\n",
    "    left_shoulder, left_hip, left_knee, left_ankle = map(lambda lm: get_point(landmarks, lm), [mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.LEFT_HIP, \n",
    "                                                                    mp_pose.PoseLandmark.LEFT_KNEE, mp_pose.PoseLandmark.LEFT_ANKLE])\n",
    "    right_shoulder, right_hip, right_knee, right_ankle = map(lambda lm: get_point(landmarks, lm), [mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.RIGHT_HIP, \n",
    "                                                                        mp_pose.PoseLandmark.RIGHT_KNEE, mp_pose.PoseLandmark.RIGHT_ANKLE])\n",
    "\n",
    "    \n",
    "    # Analyse hip posture\n",
    "    # Calculate hip angles\n",
    "    left_hip_angle = abs(calculate_angle(left_shoulder, left_hip, [left_hip[0], 0], True))\n",
    "    right_hip_angle = abs(calculate_angle(right_shoulder, right_hip, [right_hip[0], 0], True))\n",
    "\n",
    "    # Analyse ankle posture\n",
    "    # Calculate ankle angles\n",
    "    left_ankle_angle = abs(calculate_angle(left_knee, left_ankle, [left_ankle[0], 0], True))\n",
    "    right_ankle_angle = abs(calculate_angle(right_knee, right_ankle, [right_ankle[0], 0], True))\n",
    "    \n",
    "    # Determine if hip-shoulder and ankle-knee lines are asymmetric\n",
    "    left_asymmetric = abs(left_hip_angle - left_ankle_angle) > buffer + 15\n",
    "    right_asymmetric = abs(right_hip_angle - right_ankle_angle) > buffer + 15\n",
    "    \n",
    "    \n",
    "    # Analyse knee posture\n",
    "    # Calculate knee angles, subtract 180 for reflection\n",
    "    left_knee_angle = calculate_angle(left_hip, left_knee, left_ankle)\n",
    "    right_knee_angle = calculate_angle(right_hip, right_knee, right_ankle)\n",
    "    \n",
    "    # Determine if knee angles are correct\n",
    "    correct_knee_angle = 38\n",
    "    knee_low = any([abs(left_knee_angle) > (correct_knee_angle - buffer), abs(right_knee_angle) > (correct_knee_angle - buffer)])\n",
    "\n",
    "\n",
    "\n",
    "    # update posture condition\n",
    "    asym_postures = [\"L. body asym\", \"R. body asym\"]\n",
    "    knee_postures = [\"Low knees\"]\n",
    "    \n",
    "    posture = \"\"\n",
    "    if left_asymmetric:\n",
    "        posture += sign + asym_postures[0]\n",
    "    if right_asymmetric:\n",
    "        posture += sign + asym_postures[1]\n",
    "    if knee_low and (abs(left_knee_angle) < correct_knee_angle and abs(right_knee_angle) < correct_knee_angle):\n",
    "        posture += sign + knee_postures[0]\n",
    "    if posture == \"\":\n",
    "        posture = \"Correct\"\n",
    "        \n",
    "    detected_postures = posture.split(sign)\n",
    "    for detected_posture in detected_postures:\n",
    "        detected_posture = detected_posture.strip()\n",
    "        if detected_posture in asym_postures:\n",
    "            update_posture_data(\"Symmetry\", asym_postures, detected_posture)\n",
    "        elif detected_posture in knee_postures:\n",
    "            update_posture_data(\"Knee\", knee_postures, detected_posture)\n",
    "        elif detected_posture == \"Correct\":\n",
    "            posture_data[\"Correct\"] = posture_data.get(\"Correct\", 0) + 1\n",
    "\n",
    "        \n",
    "    return posture, (left_knee_angle, right_knee_angle), (left_hip_angle, right_hip_angle, left_ankle_angle, right_ankle_angle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bcf3c7",
   "metadata": {},
   "source": [
    "### 1.2.2 Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a1df32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squat_side_detection(image, results, posture, angles, body_tilts):\n",
    "    \"\"\"\n",
    "    Analyzes and annotates a side-view image of a squatting posture.\n",
    "\n",
    "    Args:\n",
    "    - image (numpy.ndarray): The image on which to perform the analysis and annotations.\n",
    "    - results (object): The detected pose landmarks from a pose estimation model.\n",
    "    - posture (str): Description of the overall posture.\n",
    "    - angles (list): A list of knee angles.\n",
    "    - body_tilts (list): List of body tilts (left and right hip and ankle angles).\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The annotated image with landmarks and angles.\n",
    "    \"\"\"\n",
    "    if results.pose_landmarks:\n",
    "        # Specify the landmarks to draw\n",
    "        landmarks_to_draw = [\n",
    "            mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.LEFT_ELBOW, mp_pose.PoseLandmark.LEFT_WRIST, mp_pose.PoseLandmark.LEFT_HIP,  \n",
    "            mp_pose.PoseLandmark.LEFT_KNEE, mp_pose.PoseLandmark.LEFT_ANKLE, mp_pose.PoseLandmark.LEFT_FOOT_INDEX,\n",
    "            mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.RIGHT_ELBOW, mp_pose.PoseLandmark.RIGHT_WRIST, mp_pose.PoseLandmark.RIGHT_HIP, \n",
    "            mp_pose.PoseLandmark.RIGHT_KNEE, mp_pose.PoseLandmark.RIGHT_ANKLE,\n",
    "        ]\n",
    "\n",
    "        # Draw the lines with specified colors\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[0], landmarks_to_draw[3], color=(0, 0, 255))\n",
    "        \n",
    "        draw_colored_connection(image, results, landmarks_to_draw[3], landmarks_to_draw[4])\n",
    "        draw_colored_connection(image, results, mp_pose.PoseLandmark.LEFT_WRIST, mp_pose.PoseLandmark.LEFT_SHOULDER)\n",
    "        draw_colored_connection(image, results, mp_pose.PoseLandmark.LEFT_KNEE, mp_pose.PoseLandmark.LEFT_ANKLE, color=(0, 0, 255))\n",
    "        draw_colored_connection(image, results, mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.RIGHT_HIP, color=(0, 0, 255))\n",
    "        draw_colored_connection(image, results, mp_pose.PoseLandmark.RIGHT_HIP, mp_pose.PoseLandmark.RIGHT_KNEE)\n",
    "        draw_colored_connection(image, results, mp_pose.PoseLandmark.RIGHT_KNEE, mp_pose.PoseLandmark.RIGHT_ANKLE, color=(0, 0, 255))\n",
    "        draw_colored_connection(image, results, mp_pose.PoseLandmark.RIGHT_WRIST, mp_pose.PoseLandmark.RIGHT_SHOULDER)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Display angles for knees, hips, and ankles\n",
    "        for side_index, angle in enumerate(angles):\n",
    "            knee_landmark = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_KNEE if side_index == 0 else mp_pose.PoseLandmark.RIGHT_KNEE]\n",
    "            cv2.putText(image, f\"{'Left' if side_index == 0 else 'Right'} Knee: {abs(round(angle, 1))}\", \n",
    "                        (int(knee_landmark.x * image.shape[1]), int(knee_landmark.y * image.shape[0] - 10)), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "        # Display body tilts\n",
    "        tilt_labels = ['LUBT', 'RUBT', 'LLBT', 'RLBT']\n",
    "        for index, tilt in enumerate(body_tilts):\n",
    "            tilt_point = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP if index < 2 else mp_pose.PoseLandmark.LEFT_ANKLE]\n",
    "            cv2.putText(image, f\"{tilt_labels[index]}: {round(tilt, 1)}\", \n",
    "                        (int(tilt_point.x * image.shape[1]), int(tilt_point.y * image.shape[0] + 20 * (index % 2))), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "        # Add text for knee position if necessary\n",
    "        cv2.putText(image, f'Posture: {posture}', (0, 20), cv2.FONT_HERSHEY_COMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5a5858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squat_back_detection(image, results, posture, angles):\n",
    "    if results.pose_landmarks:\n",
    "        # Specify the landmarks to draw\n",
    "        landmarks_to_draw = [\n",
    "            mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.LEFT_HIP, \n",
    "            mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.RIGHT_HIP\n",
    "        ]\n",
    "        \n",
    "        # Draw lines between shoulders and hips\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[0], landmarks_to_draw[2])\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[1], landmarks_to_draw[3])\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[0], landmarks_to_draw[1], color=(0, 0, 255))\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[2], landmarks_to_draw[3], color=(0, 0, 255))\n",
    "\n",
    "        \n",
    "        # Draw the specified landmarks\n",
    "        for landmark in landmarks_to_draw:\n",
    "            landmark_point = results.pose_landmarks.landmark[landmark]\n",
    "            pos = (int(landmark_point.x * image.shape[1]), int(landmark_point.y * image.shape[0]))\n",
    "            cv2.circle(image, pos, 3, (255, 255, 255), -1)\n",
    "\n",
    "        # Draw the lines and display angles\n",
    "        joint_names = [\"Shoulder Dev\", \"Hip Dev\"]\n",
    "        joint_landmarks = [landmark.value for landmark in landmarks_to_draw[:2]]\n",
    "\n",
    "\n",
    "        # Call the draw_labeled_box function\n",
    "        image = draw_labeled_box(image, results, joint_landmarks, joint_names, angles)\n",
    "\n",
    "        # Add text for posture\n",
    "        cv2.putText(image, f'Posture: {posture}', (20, 50), cv2.FONT_HERSHEY_COMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf9e531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squat side\n",
    "run_estimation(squat_side_analysis, squat_side_detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f7602b",
   "metadata": {},
   "source": [
    "## 1.3 Squat Back "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065b7108",
   "metadata": {},
   "source": [
    "### 1.3.1 Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacb512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squat_back_analysis(landmarks, mp_pose):\n",
    "    \"\"\"\n",
    "    Analyzes back-view squat posture based on landmarks.\n",
    "\n",
    "    Args:\n",
    "    - landmarks (list): List of detected pose landmarks.\n",
    "    - mp_pose (module): Mediapipe pose module for landmark references.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Returns a tuple containing the detected posture and a tuple of angles (shoulder angle difference, hip angle difference).\n",
    "    \"\"\"\n",
    "\n",
    "    # Get points for shoulders and hips\n",
    "    left_shoulder, left_hip = map(lambda lm: get_point(landmarks, lm), [pose_landmark.LEFT_SHOULDER, pose_landmark.LEFT_HIP])\n",
    "    right_shoulder, right_hip = map(lambda lm: get_point(landmarks, lm), [pose_landmark.RIGHT_SHOULDER, pose_landmark.RIGHT_HIP])\n",
    "    \n",
    "    # Posture Analysis\n",
    "    # Calculate shoulder angles\n",
    "    left_shoulder_angle = abs(calculate_angle(right_shoulder, left_shoulder, [left_shoulder[0], 0]))\n",
    "    right_shoulder_angle = abs(calculate_angle(left_shoulder, right_shoulder, [right_shoulder[0], 0]))\n",
    "    \n",
    "    # Determine if shoulders are level\n",
    "    left_shoulder_high = left_shoulder_angle - right_shoulder_angle > sml_buffer\n",
    "    right_shoulder_high = right_shoulder_angle - left_shoulder_angle > sml_buffer\n",
    "\n",
    "     # Calculate hip angles\n",
    "    left_hip_angle = abs(calculate_angle(right_hip, left_hip, [left_hip[0], 0]))\n",
    "    right_hip_angle = abs(calculate_angle(left_hip, right_hip, [right_hip[0], 0]))\n",
    "    \n",
    "    # Determine if hips are level\n",
    "    left_hip_high = left_hip_angle - right_hip_angle > sml_buffer\n",
    "    right_hip_high = right_hip_angle - left_hip_angle > sml_buffer\n",
    "    \n",
    "\n",
    "    # Update posture condition\n",
    "    shoulder_postures = [\"H.L. shoulder\", \"H.R. shoulder\"]\n",
    "    hip_postures = [\"H.L. hip\", \"H.R. hip\"]\n",
    "    posture = \"\"\n",
    "    if left_shoulder_high:\n",
    "        posture += sign + shoulder_postures[0]\n",
    "    elif right_shoulder_high:\n",
    "        posture += sign + shoulder_postures[1]\n",
    "    if left_hip_high:\n",
    "        posture += sign + hip_postures[0]\n",
    "    elif right_hip_high:\n",
    "        posture += sign + hip_postures[1]\n",
    "    if posture == \"\":\n",
    "        posture = \"Correct\"\n",
    "\n",
    "    detected_postures = posture.split(sign)\n",
    "    for detected_posture in detected_postures:\n",
    "        detected_posture = detected_posture.strip()\n",
    "        if detected_posture in shoulder_postures:\n",
    "            update_posture_data(\"Shoulder\", shoulder_postures, detected_posture)\n",
    "        elif detected_posture in hip_postures:\n",
    "            update_posture_data(\"Hip\", hip_postures, detected_posture)\n",
    "        elif detected_posture == \"Correct\":\n",
    "            posture_data[\"Correct\"] = posture_data.get(\"Correct\", 0) + 1     \n",
    "        \n",
    "        \n",
    "    return posture, (left_shoulder_angle - right_shoulder_angle, left_hip_angle - right_hip_angle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41587520",
   "metadata": {},
   "source": [
    "### 1.3.2 Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05630747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squat_back_detection(image, results, posture, angles):\n",
    "    if results.pose_landmarks:\n",
    "        # Specify the landmarks to draw\n",
    "        landmarks_to_draw = [\n",
    "            mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.LEFT_HIP, \n",
    "            mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.RIGHT_HIP\n",
    "        ]\n",
    "        \n",
    "        # Draw lines between shoulders and hips\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[0], landmarks_to_draw[2])\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[1], landmarks_to_draw[3])\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[0], landmarks_to_draw[1], color=(0, 0, 255))\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[2], landmarks_to_draw[3], color=(0, 0, 255))\n",
    "\n",
    "        \n",
    "        # Draw the specified landmarks\n",
    "        for landmark in landmarks_to_draw:\n",
    "            landmark_point = results.pose_landmarks.landmark[landmark]\n",
    "            pos = (int(landmark_point.x * image.shape[1]), int(landmark_point.y * image.shape[0]))\n",
    "            cv2.circle(image, pos, 3, (255, 255, 255), -1)\n",
    "\n",
    "        # Draw the lines and display angles\n",
    "        joint_names = [\"Shoulder Dev\", \"Hip Dev\"]\n",
    "        joint_landmarks = [landmark.value for landmark in landmarks_to_draw[:2]]\n",
    "\n",
    "\n",
    "        # Call the draw_labeled_box function\n",
    "        image = draw_labeled_box(image, results, joint_landmarks, joint_names, angles)\n",
    "\n",
    "        # Add text for posture\n",
    "        cv2.putText(image, f'Posture: {posture}', (20, 50), cv2.FONT_HERSHEY_COMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7338b0b",
   "metadata": {},
   "source": [
    "## 2. Knee Raising"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19529e1a",
   "metadata": {},
   "source": [
    "### 2.0.1 Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c3967ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knee_raising_analysis(landmarks, mp_pose):\n",
    "    \"\"\"\n",
    "    Analyzes posture during knee-raising exercises based on landmarks.\n",
    "\n",
    "    Args:\n",
    "    - landmarks (list): List of detected pose landmarks.\n",
    "    - mp_pose (module): Mediapipe pose module for landmark references.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Returns a tuple containing the detected posture and a list with shoulder angle difference, shoulder midpoint x-coordinate, and central vertical line x-coordinate.\n",
    "    \"\"\"\n",
    "    # Get points for shoulders and hips\n",
    "    left_shoulder, left_hip = map(lambda lm: get_point(landmarks, lm), [pose_landmark.LEFT_SHOULDER, pose_landmark.LEFT_HIP])\n",
    "    right_shoulder, right_hip = map(lambda lm: get_point(landmarks, lm), [pose_landmark.RIGHT_SHOULDER, pose_landmark.RIGHT_HIP])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Posture Analysis\n",
    "    # Calculate shoulder angles\n",
    "    left_shoulder_angle = abs(calculate_angle(right_shoulder, left_shoulder, [left_shoulder[0], 0]))\n",
    "    right_shoulder_angle = abs(calculate_angle(left_shoulder, right_shoulder, [right_shoulder[0], 0]))\n",
    "    \n",
    "    # Determine if shoulders are level\n",
    "    left_shoulder_high = left_shoulder_angle - right_shoulder_angle > sml_buffer\n",
    "    right_shoulder_high = right_shoulder_angle - left_shoulder_angle > sml_buffer\n",
    "\n",
    "    \n",
    "    \n",
    "    # Calculate the midpoint of the shoulders\n",
    "    shoulder_mid_x = (left_shoulder[0] + right_shoulder[0]) / 2\n",
    "    # Calculate the central vertical line (midpoint between hips)\n",
    "    central_vertical_line_x = (left_hip[0] + right_hip[0]) / 2\n",
    "    \n",
    "    mp_buffer = 0.025 # set buffer for midpoint\n",
    "    # Determine if the shoulder mid point is over the left or right of the hip mid point\n",
    "    mid_point_left = (shoulder_mid_x - central_vertical_line_x) < -mp_buffer\n",
    "    mid_point_right = (shoulder_mid_x - central_vertical_line_x) > mp_buffer\n",
    "    #print((shoulder_mid_x - central_vertical_line_x))\n",
    "    # Update posture condition\n",
    "    shoulder_postures = [\"H.L. shoulder\", \"H.R. shoulder\"]\n",
    "    mp_postures = [\"Body too left\", \"Body too right\"]\n",
    "    \n",
    "    posture = \"\"\n",
    "    if left_shoulder_high:\n",
    "        posture += sign + shoulder_postures[0]\n",
    "    elif right_shoulder_high:\n",
    "        posture += sign + shoulder_postures[1]\n",
    "    if mid_point_left:\n",
    "        posture += sign + mp_postures[0]\n",
    "    elif mid_point_right:\n",
    "        posture += sign + mp_postures[1]\n",
    "    if posture == \"\":\n",
    "        posture = \"Correct\"\n",
    "\n",
    "    detected_postures = posture.split(sign)\n",
    "    for detected_posture in detected_postures:\n",
    "        detected_posture = detected_posture.strip()\n",
    "        if detected_posture in shoulder_postures:\n",
    "            update_posture_data(\"Shoulder\", shoulder_postures, detected_posture)\n",
    "        elif detected_posture in mp_postures:\n",
    "            update_posture_data(\"Mid Point\", mp_postures, detected_posture)\n",
    "        elif detected_posture == \"Correct\":\n",
    "            posture_data[\"Correct\"] = posture_data.get(\"Correct\", 0) + 1 \n",
    "\n",
    "    return posture, [left_shoulder_angle - right_shoulder_angle, shoulder_mid_x, central_vertical_line_x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bf3764",
   "metadata": {},
   "source": [
    "### 2.0.2 Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fda23ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knee_raising_detection(image, results, posture, analysis_results):\n",
    "    \"\"\"\n",
    "    Analyzes and annotates an image for knee raising exercises.\n",
    "\n",
    "    Args:\n",
    "    - image (numpy.ndarray): The image on which to perform the analysis and annotations.\n",
    "    - results (object): The detected pose landmarks from a pose estimation model.\n",
    "    - posture (str): Description of the overall posture.\n",
    "    - analysis_results (tuple): Contains analysis data like shoulder vertical difference,\n",
    "                                shoulder midpoint, and central vertical line position.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The annotated image with landmarks, vertical line, and deviation information.\n",
    "    \"\"\"\n",
    "    shoulder_vertical_difference, shoulder_mid_x, central_vertical_line_x = analysis_results\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        landmark_idx = [11, 12, 23, 24]\n",
    "        \n",
    "        # Draw the lines\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[landmark_idx[0]], landmarks_to_draw[landmark_idx[1]])\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[landmark_idx[2]], landmarks_to_draw[landmark_idx[3]], color=(0, 0, 255))\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[landmark_idx[0]], landmarks_to_draw[landmark_idx[2]], color=(0, 0, 255))\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[landmark_idx[1]], landmarks_to_draw[landmark_idx[3]], color=(0, 0, 255))\n",
    "\n",
    "        \n",
    "        # Draw central vertical line\n",
    "        central_vertical_line_pixel_x = int(central_vertical_line_x * image.shape[1])\n",
    "        cv2.line(image, (central_vertical_line_pixel_x, 0), (central_vertical_line_pixel_x, image.shape[0]), (255, 255, 0), 1)\n",
    "\n",
    "        # Calculate middle dot position (midpoint between shoulders)\n",
    "        middle_dot_x = int(shoulder_mid_x * image.shape[1])\n",
    "        middle_dot_y = int((results.pose_landmarks.landmark[landmarks_to_draw[landmark_idx[0]]].y +\n",
    "                            results.pose_landmarks.landmark[landmarks_to_draw[landmark_idx[1]]].y) / 2 * image.shape[0])\n",
    "        \n",
    "        # Draw the specified landmarks\n",
    "        for idx in landmark_idx:\n",
    "            landmark_point = results.pose_landmarks.landmark[landmarks_to_draw[idx]]\n",
    "            pos = (int(landmark_point.x * image.shape[1]), int(landmark_point.y * image.shape[0]))\n",
    "            cv2.circle(image, pos, 3, (255, 255, 255), -1)\n",
    "            \n",
    "        # Draw the middle dot\n",
    "        cv2.circle(image, (middle_dot_x, middle_dot_y), 3, (0, 255, 0), -1)  # Green dot\n",
    "        \n",
    "        # Calculate positions for drawing text\n",
    "        shoulder_midpoint_pos = (middle_dot_x, middle_dot_y)\n",
    "    \n",
    "        # For \"Shoulder Diff\" text\n",
    "        shoulder_diff_text = f'Diff: {round(abs(shoulder_vertical_difference), 1)}'\n",
    "        deviation_text = 'Left' if (shoulder_mid_x - central_vertical_line_x) > 0 else 'Right' if (shoulder_mid_x - central_vertical_line_x) < 0 else 'Centered'\n",
    "        deviation_full_text = f'Dev: {deviation_text}'\n",
    "\n",
    "        # Calculate size of the text for background box calculation\n",
    "        shoulder_diff_text_size = cv2.getTextSize(shoulder_diff_text, cv2.FONT_HERSHEY_SIMPLEX, 0.3, 1)[0]\n",
    "        deviation_text_size = cv2.getTextSize(deviation_full_text, cv2.FONT_HERSHEY_SIMPLEX, 0.3, 1)[0]\n",
    "\n",
    "        # Shoulder Diff Box\n",
    "        shoulder_diff_box_start = (shoulder_midpoint_pos[0] + 5, shoulder_midpoint_pos[1] - 5 - shoulder_diff_text_size[1] - 2)\n",
    "        shoulder_diff_box_end = (shoulder_diff_box_start[0] + shoulder_diff_text_size[0] + 4, shoulder_midpoint_pos[1] - 5 + 2)\n",
    "        cv2.rectangle(image, shoulder_diff_box_start, shoulder_diff_box_end, (255, 255, 255), cv2.FILLED)\n",
    "        cv2.rectangle(image, shoulder_diff_box_start, shoulder_diff_box_end, (230, 216, 173), 1)\n",
    "\n",
    "        # Deviation Box\n",
    "        deviation_box_start = (central_vertical_line_pixel_x + 5, shoulder_midpoint_pos[1] + 15 - deviation_text_size[1] - 2)\n",
    "        deviation_box_end = (deviation_box_start[0] + deviation_text_size[0] + 4, shoulder_midpoint_pos[1] + 15 + 2)\n",
    "        cv2.rectangle(image, deviation_box_start, deviation_box_end, (255, 255, 255), cv2.FILLED)\n",
    "        cv2.rectangle(image, deviation_box_start, deviation_box_end, (230, 216, 173), 1)\n",
    "\n",
    "        # Now put the text on top of the boxes\n",
    "        cv2.putText(image, shoulder_diff_text, (shoulder_diff_box_start[0], shoulder_diff_box_start[1] + shoulder_diff_text_size[1] + 2), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (139, 0, 0), 1)\n",
    "        cv2.putText(image, deviation_full_text, (deviation_box_start[0], deviation_box_start[1] + deviation_text_size[1] + 2), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (139, 0, 0), 1)\n",
    "        \n",
    "        \n",
    "        # Add text for posture\n",
    "        cv2.putText(image, f'Posture: {posture}', (10, 30), cv2.FONT_HERSHEY_COMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e0ece276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter 1 for image/video file, 2 for camera: 1\n",
      "Enter the path of the image or video file: 1.mp4\n",
      "Path for output video (.avi or .mp4): \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Section</th>\n",
       "      <th>Condition</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shoulder</td>\n",
       "      <td>H.L. shoulder</td>\n",
       "      <td>100.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mid Point</td>\n",
       "      <td>Body too right</td>\n",
       "      <td>100.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shoulder Issue</td>\n",
       "      <td></td>\n",
       "      <td>52.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mid Point Issue</td>\n",
       "      <td></td>\n",
       "      <td>7.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Correct Ratio</td>\n",
       "      <td></td>\n",
       "      <td>40.7%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Section       Condition Percentage\n",
       "0         Shoulder   H.L. shoulder     100.0%\n",
       "1        Mid Point  Body too right     100.0%\n",
       "2   Shoulder Issue                      52.0%\n",
       "3  Mid Point Issue                       7.3%\n",
       "4    Correct Ratio                      40.7%"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Knee raising\n",
    "run_estimation(knee_raising_analysis, knee_raising_detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c73bd4",
   "metadata": {},
   "source": [
    "## 3.1 Stand front"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968c33d3",
   "metadata": {},
   "source": [
    "### 3.1.1 Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d27ca8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stand_front_analysis(landmarks, mp_pose):\n",
    "    \"\"\"\n",
    "    Analyzes front-view standing posture based on landmarks.\n",
    "\n",
    "    Args:\n",
    "    - landmarks (list): List of detected pose landmarks.\n",
    "    - mp_pose (module): Mediapipe pose module for landmark references.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Returns a tuple containing the detected posture and a tuple of angle differences for ears, shoulders, hips, knees, and ankles.\n",
    "    \"\"\"\n",
    "    # Get points for ears, shoulders, hips, knees, and ankles\n",
    "    left_ear, left_shoulder, left_hip, left_knee, left_ankle = map(\n",
    "        lambda lm: get_point(landmarks, lm),\n",
    "        [mp_pose.PoseLandmark.LEFT_EAR, mp_pose.PoseLandmark.LEFT_SHOULDER, \n",
    "         mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.LEFT_KNEE, \n",
    "         mp_pose.PoseLandmark.LEFT_ANKLE])\n",
    "\n",
    "    right_ear, right_shoulder, right_hip, right_knee, right_ankle = map(\n",
    "        lambda lm: get_point(landmarks, lm),\n",
    "        [mp_pose.PoseLandmark.RIGHT_EAR, mp_pose.PoseLandmark.RIGHT_SHOULDER, \n",
    "         mp_pose.PoseLandmark.RIGHT_HIP, mp_pose.PoseLandmark.RIGHT_KNEE, \n",
    "         mp_pose.PoseLandmark.RIGHT_ANKLE])\n",
    "\n",
    "    # Calculate joint angles\n",
    "    left_ear_angle = abs(calculate_angle([left_ear[0], 0], left_ear, right_ear))\n",
    "    right_ear_angle = abs(calculate_angle([right_ear[0], 0], right_ear, left_ear))\n",
    "\n",
    "    left_shoulder_angle = abs(calculate_angle([left_shoulder[0], 0], left_shoulder, right_shoulder))\n",
    "    right_shoulder_angle = abs(calculate_angle([right_shoulder[0], 0], right_shoulder, left_shoulder))\n",
    "\n",
    "    left_hip_angle = abs(calculate_angle([left_hip[0], 0], left_hip, right_hip))\n",
    "    right_hip_angle = abs(calculate_angle([right_hip[0], 0], right_hip, left_hip))\n",
    "\n",
    "    left_knee_angle = abs(calculate_angle([left_knee[0], 0], left_knee, right_knee))\n",
    "    right_knee_angle = abs(calculate_angle([right_knee[0], 0], right_knee, left_knee))\n",
    "\n",
    "    left_ankle_angle = abs(calculate_angle([left_ankle[0], 0], left_ankle, right_ankle))\n",
    "    right_ankle_angle = abs(calculate_angle([right_ankle[0], 0], right_ankle, left_ankle))\n",
    "\n",
    "    # Determine if joints are level using angle differences and buffer\n",
    "\n",
    "    ear_postures = [\"H.L. ear\", \"H.R. ear\"]\n",
    "    shoulder_postures = [\"H.L. shoulder\", \"H.R. shoulder\"]\n",
    "    hip_postures = [\"H.L. hip\", \"H.R. hip\"]\n",
    "    knee_postures = [\"H.L. knee\", \"H.R. knee\"]\n",
    "    ankle_postures = [\"H.L. ankle\", \"H.R. ankle\"]\n",
    "\n",
    "    posture = \"\"\n",
    "    if left_ear_angle - right_ear_angle > sml_buffer:\n",
    "        posture += sign + ear_postures[0]\n",
    "    elif right_ear_angle - left_ear_angle > sml_buffer:\n",
    "        posture += sign + ear_postures[1]\n",
    "\n",
    "    if left_shoulder_angle - right_shoulder_angle > sml_buffer:\n",
    "        posture += sign + shoulder_postures[0]\n",
    "    elif right_shoulder_angle - left_shoulder_angle > sml_buffer:\n",
    "        posture += sign + shoulder_postures[1]\n",
    "\n",
    "    if left_hip_angle - right_hip_angle > sml_buffer:\n",
    "        posture += sign + hip_postures[0]\n",
    "    elif right_hip_angle - left_hip_angle > sml_buffer:\n",
    "        posture += sign + hip_postures[1]\n",
    "\n",
    "    if left_knee_angle - right_knee_angle > sml_buffer:\n",
    "        posture += sign + knee_postures[0]\n",
    "    elif right_knee_angle - left_knee_angle > sml_buffer:\n",
    "        posture += sign + knee_postures[1]\n",
    "\n",
    "    if left_ankle_angle - right_ankle_angle > sml_buffer:\n",
    "        posture += sign + ankle_postures[0]\n",
    "    elif right_ankle_angle - left_ankle_angle > sml_buffer:\n",
    "        posture += sign + ankle_postures[1]\n",
    "\n",
    "    if posture == \"\":\n",
    "        posture = \"Correct\"\n",
    "\n",
    "    # Update posture data\n",
    "    detected_postures = posture.split(sign)\n",
    "    for detected_posture in detected_postures:\n",
    "        detected_posture = detected_posture.strip()\n",
    "        if detected_posture in ear_postures:\n",
    "            update_posture_data(\"Ear\", ear_postures, detected_posture)\n",
    "        elif detected_posture in shoulder_postures:\n",
    "            update_posture_data(\"Shoulder\", shoulder_postures, detected_posture)\n",
    "        elif detected_posture in hip_postures:\n",
    "            update_posture_data(\"Hip\", hip_postures, detected_posture)\n",
    "        elif detected_posture in knee_postures:\n",
    "            update_posture_data(\"Knee\", knee_postures, detected_posture)\n",
    "        elif detected_posture in ankle_postures:\n",
    "            update_posture_data(\"Ankle\", ankle_postures, detected_posture)\n",
    "        elif detected_posture == \"Correct\":\n",
    "            posture_data[\"Correct\"] = posture_data.get(\"Correct\", 0) + 1\n",
    "\n",
    "    return posture, (left_ear_angle - right_ear_angle, left_shoulder_angle - right_shoulder_angle, left_hip_angle - right_hip_angle, left_knee_angle - right_knee_angle, left_ankle_angle - right_ankle_angle)\n",
    "\n",
    "# Define or import calculate_angle, posture_data, update_posture_data, mp_pose, and pose_landmark before using this function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735b3cac",
   "metadata": {},
   "source": [
    "### 3.1.2 Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "afb80d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stand_front_detection(image, results, posture, angles):\n",
    "    \"\"\"\n",
    "    Analyzes and annotates an image for stand front exercises.\n",
    "\n",
    "    Args:\n",
    "    - image (numpy.ndarray): The image on which to perform the analysis and annotations.\n",
    "    - results (object): The detected pose landmarks from a pose estimation model.\n",
    "    - posture (str): Description of the overall posture.\n",
    "    - analysis_results (tuple): Contains analysis data like shoulder vertical difference,\n",
    "                                shoulder midpoint, and central vertical line position.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The annotated image with landmarks, vertical line, and deviation information.\n",
    "    \"\"\"\n",
    "    \n",
    "    if results.pose_landmarks:\n",
    "        landmark_idx = [7, 8, 11, 12, 23, 24, 25, 26, 27, 28]\n",
    "        # Draw the lines\n",
    "        for i in range(0, len(landmark_idx), 2):\n",
    "            draw_colored_connection(image, results, landmarks_to_draw[landmark_idx[i]], landmarks_to_draw[landmark_idx[i+1]])\n",
    "        for i in range(2, len(landmark_idx)-2, 2):\n",
    "            draw_colored_connection(image, results, landmarks_to_draw[landmark_idx[i]], landmarks_to_draw[landmark_idx[i+2]], color=(0, 0, 255))\n",
    "        for i in range(3, len(landmark_idx)-2, 2):\n",
    "            draw_colored_connection(image, results, landmarks_to_draw[landmark_idx[i]], landmarks_to_draw[landmark_idx[i+2]], color=(0, 0, 255))\n",
    "\n",
    "        # Draw the specified landmarks\n",
    "        for idx in landmark_idx:\n",
    "            landmark_point = results.pose_landmarks.landmark[landmarks_to_draw[idx]]\n",
    "            pos = (int(landmark_point.x * image.shape[1]), int(landmark_point.y * image.shape[0]))\n",
    "            cv2.circle(image, pos, 3, (255, 255, 255), -1)\n",
    "\n",
    "        # Draw the lines and display angles\n",
    "        joint_names = [\"Ear\", \"Shoulder\", \"Hip\", \"Knee\", \"Ankle\"]\n",
    "        joint_landmarks = [landmarks_to_draw[idx].value for idx in landmark_idx[::2]]\n",
    "\n",
    "        # Call the draw_labeled_box function\n",
    "        image = draw_labeled_box(image, results, joint_landmarks, joint_names, angles)\n",
    "        \n",
    "        # Add text for posture\n",
    "        cv2.putText(image, f'Posture: {posture}', (0, 20), cv2.FONT_HERSHEY_COMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1866ff6c",
   "metadata": {},
   "source": [
    "## 3.2 Stand Side"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636d94c0",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4081dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stand_side_analysis(landmarks, mp_pose):\n",
    "    \"\"\"\n",
    "    Analyzes side-view standing posture based on landmarks.\n",
    "\n",
    "    Args:\n",
    "    - landmarks (list): List of detected pose landmarks.\n",
    "    - mp_pose (module): Mediapipe pose module for landmark references.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Returns a tuple containing the detected posture and the left shoulder angle.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get points for ear and shoulder\n",
    "    left_ear, left_shoulder, left_hip, left_knee, left_ankle = map(\n",
    "        lambda lm: get_point(landmarks, lm),\n",
    "        [mp_pose.PoseLandmark.LEFT_EAR, mp_pose.PoseLandmark.LEFT_SHOULDER, \n",
    "         mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.LEFT_KNEE, mp_pose.PoseLandmark.LEFT_ANKLE])\n",
    "\n",
    "    # Posture Analysis\n",
    "    # Calculate shoulder angle\n",
    "    left_shoulder_angle = abs(calculate_angle([left_shoulder[0], 0], left_shoulder, left_ear))\n",
    "    \n",
    "    left_knee_angle = calculate_angle(left_hip, left_knee, left_ankle)\n",
    "\n",
    "    # Determine if shoulder angle is correct\n",
    "    correct_shoulder_angle = 0  # Correct shoulder angle (ideal posture)\n",
    "    shoulder_high = left_shoulder_angle > (correct_shoulder_angle + buffer)\n",
    "\n",
    "    # Update posture condition\n",
    "    shoulder_postures = [\"Forward head\"]\n",
    "    posture = \"\"\n",
    "    if shoulder_high:\n",
    "        posture += sign + shoulder_postures[0]\n",
    "\n",
    "\n",
    "    if posture == \"\":\n",
    "        posture = \"Correct\"\n",
    "\n",
    "    # Update posture data\n",
    "    detected_posture = posture.strip(sign)\n",
    "    if detected_posture in shoulder_postures:\n",
    "        update_posture_data(\"Head\", shoulder_postures, detected_posture)\n",
    "    elif detected_posture == \"Correct\":\n",
    "        posture_data[\"Correct\"] = posture_data.get(\"Correct\", 0) + 1\n",
    "\n",
    "    return posture, (left_shoulder_angle, left_knee_angle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a3ca1d",
   "metadata": {},
   "source": [
    "### 3.2.2 Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "00389f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stand_side_detection(image, results, posture, angles):\n",
    "    \"\"\"\n",
    "    Analyzes and annotates a side-view image of a standing posture.\n",
    "\n",
    "    Args:\n",
    "    - image (numpy.ndarray): The image on which to perform the analysis and annotations.\n",
    "    - results (object): The detected pose landmarks from a pose estimation model.\n",
    "    - posture (str): Description of the overall posture.\n",
    "    - angles (tuple): Tuple containing angles, where angles[0] is the neck angle and angles[1] is another angle, such as the knee angle.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The annotated image with neck angle and posture information.\n",
    "    \"\"\"\n",
    "    if results.pose_landmarks:\n",
    "        landmark_idx = [7, 11, 23, 25, 27]\n",
    "        # Draw the lines\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[7], landmarks_to_draw[11])\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[11], landmarks_to_draw[23], color=(0, 0, 255))\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[23], landmarks_to_draw[25])\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[25], landmarks_to_draw[27], color=(0, 0, 255))\n",
    "        \n",
    "        # Draw a vertical line across the left hip\n",
    "        left_hip_landmark = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP.value]\n",
    "        left_hip_x = int(left_hip_landmark.x * image.shape[1])\n",
    "        cv2.line(image, (left_hip_x, 0), (left_hip_x, image.shape[0]), (255, 255, 0), thickness=1)\n",
    "        \n",
    "        # Draw circles for specified landmarks\n",
    "        \n",
    "        for idx in landmark_idx:\n",
    "            landmark_point = results.pose_landmarks.landmark[landmarks_to_draw[idx]]\n",
    "            pos = (int(landmark_point.x * image.shape[1]), int(landmark_point.y * image.shape[0]))\n",
    "            cv2.circle(image, pos, 3, (255, 255, 255), -1)\n",
    "            \n",
    "        # Display angle information using a loop\n",
    "        joint_names = ['Neck', 'Knee']  # Add more joint names as needed\n",
    "        joint_landmarks = [landmarks_to_draw[11].value, landmarks_to_draw[25].value]\n",
    "\n",
    "        # Call the draw_labeled_box function\n",
    "        image = draw_labeled_box(image, results, joint_landmarks, joint_names, angles)\n",
    "\n",
    "        # Add text for posture\n",
    "        cv2.putText(image, f'Posture: {posture}', (10, 20), cv2.FONT_HERSHEY_COMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af504e45",
   "metadata": {},
   "source": [
    "# Run Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444f9a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squat front\n",
    "run_estimation(squat_front_analysis, squat_front_detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3530ebc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Squat side\n",
    "run_estimation(squat_side_analysis, squat_side_detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc3b212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squat back\n",
    "run_estimation(squat_back_analysis, squat_back_detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07716d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knee raising\n",
    "run_estimation(knee_raising_analysis, knee_raising_detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baced1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stand front\n",
    "run_estimation(stand_front_analysis, stand_front_detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a49b9a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter 1 for image/video file, 2 for camera: 1\n",
      "Enter the path of the image or video file: 4.mp4\n",
      "Path for output video (.avi or .mp4): \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Section</th>\n",
       "      <th>Condition</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Head</td>\n",
       "      <td>Forward head</td>\n",
       "      <td>100.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Head Issue</td>\n",
       "      <td></td>\n",
       "      <td>100.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Correct Ratio</td>\n",
       "      <td></td>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Section     Condition Percentage\n",
       "0           Head  Forward head     100.0%\n",
       "1     Head Issue                   100.0%\n",
       "2  Correct Ratio                     0.0%"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stand side\n",
    "run_estimation(stand_side_analysis, stand_side_detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc67bfa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
