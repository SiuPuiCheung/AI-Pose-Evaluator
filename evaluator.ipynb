{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ea836eb",
   "metadata": {
    "id": "3ea836eb"
   },
   "source": [
    "<!-- This notebook is created by Siu Pui Cheung, 09/02/2024 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4106c544",
   "metadata": {
    "id": "4106c544",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 0. Utilities and Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6687bf86-a846-4104-bf58-64f47aac62c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 0.1 Initialization and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a37294-b1e1-42f4-a9af-594ed1839758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  # For image and video processing\n",
    "import os \n",
    "import sys \n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "import tkinter as tk  \n",
    "from tkinter import filedialog, PhotoImage  # For file dialog and image handling in GUI\n",
    "import mediapipe as mp  \n",
    "import threading  # For running processes in parallel threads\n",
    "from matplotlib.gridspec import GridSpec  # For advanced plot layouts\n",
    "import matplotlib.pyplot as plt  \n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize mediapipe pose class for pose detection\n",
    "mp_pose, pose_landmark = mp.solutions.pose, mp.solutions.pose.PoseLandmark\n",
    "\n",
    "# Define a list of landmarks to draw from the Mediapipe pose landmarks\n",
    "landmarks_to_draw = list(mp_pose.PoseLandmark)\n",
    "\n",
    "# Control variables for running the main loop and evaluation flag\n",
    "run, stop_evaluation = True, False\n",
    "\n",
    "# DataFrame to store joint angles data during the analysis\n",
    "joint_angles_df = pd.DataFrame()\n",
    "\n",
    "# Define labels for body joints and their angles for different analysis scenarios\n",
    "body_labels = [\n",
    "    ['Left shoulder', 'Right Shoulder', 'Left Elbow', 'Right Elbow', 'Left Wrist', 'Right Wrist', 'Left Upper Body', 'Right Upper Body', 'Left Knee', 'Right Knee', 'Left Low Limb', 'Right Low Limb', 'Shoulder Midpoint Deviation: L(+), R(-)'],\n",
    "    ['Left Shoulder', 'Right Shoulder', 'Left Elbow', 'Right Elbow', 'Left Upper Body', 'Right Upper Body', 'Left Knee', 'Right Knee', 'Left Low Limb', 'Right Low Limb', 'Left Neck', 'Right Neck', 'Left Hip', 'Right Hip', 'Left Ankle Flexion', 'Right Ankle Flexion'],\n",
    "    ['Shoulder Angle Difference: L(+), R(-)', 'Elbow Angle Difference: L(+), R(-)', 'Hip Angle Difference: L(+), R(-)'],\n",
    "    ['Shoulder Midpoint Deviation: L(+), R(-)', 'Central Vertical Midpoint: L(+), R(-)'],\n",
    "    ['Ear Angle Difference: L(+), R(-)', 'Shoulder Angle Difference: L(+), R(-)', 'Hip Angle Difference: L(+), R(-)', 'Knee Angle Difference: L(+), R(-)', 'Ankle Angle Difference: L(+), R(-)'],\n",
    "    ['Left Shoulder Angle', 'Left Low Limb Angle']\n",
    "]\n",
    "\n",
    "# Set confidence thresholds for image and video pose detection and tracking\n",
    "d_conf_img, t_conf_img = 0.6, 0.6  # For image-based detection and tracking\n",
    "d_conf_vid, t_conf_vid = 0.9, 0.9  # For video-based detection and tracking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28800268-0b4f-47f1-a99f-4b2b9c53dea3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 0.2 GUI Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37c6a9d-3e67-4a3e-84aa-34298408d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gui(options):\n",
    "    '''\n",
    "    Creates a graphical user interface (GUI) dialog with buttons based on provided options.\n",
    "    \n",
    "    This function dynamically generates a dialog window displaying buttons for each option passed through the `options` dictionary. \n",
    "    When a button is pressed, the function returns the value associated with the selected option's key.\n",
    "    \n",
    "    Args:\n",
    "    - options (dict): A dictionary where keys are the text displayed on buttons, and values are the corresponding values returned when a button is clicked.\n",
    "    \n",
    "    Returns:\n",
    "    - The value associated with the key of the selected button. If no selection is made, returns None.\n",
    "    '''\n",
    "    # Check if the application is frozen to determine the path for resources\n",
    "    if getattr(sys, 'frozen', False):\n",
    "        # When running as an executable, the path is different\n",
    "        application_path = sys._MEIPASS  \n",
    "    else:\n",
    "        # Fallback to current working directory when running as a script\n",
    "        application_path = os.getcwd()  \n",
    "\n",
    "    # Construct the path to the background image\n",
    "    bg_image_path = os.path.join(application_path, 'image/bg.png')  \n",
    "\n",
    "    # Initialize dialog window\n",
    "    dialog = tk.Toplevel()\n",
    "    dialog.title(\"AI Posture Evaluator\")\n",
    "\n",
    "    # Load and place background image\n",
    "    bg_image = PhotoImage(file=bg_image_path).subsample(2, 2)\n",
    "    tk.Label(dialog, image=bg_image).place(x=0, y=0, relwidth=1, relheight=1)\n",
    "    dialog.bg_image = bg_image  # Keep a reference to prevent garbage collection\n",
    "    tk.Label(dialog, text=\"Â© 2024 E.C.| v2.2.1\", font=(\"Helvetica\", 5)).place(x=0, y=bg_image.height() - 10)\n",
    "    dialog.geometry(f\"{bg_image.width()}x{bg_image.height()}\")\n",
    "    dialog.grab_set()  # Grab all mouse/keyboard events in this window\n",
    "\n",
    "    # Initialize a variable to store the user's selection\n",
    "    result = [None] \n",
    "\n",
    "    # Define layout parameters for the buttons\n",
    "    params = 120, 120, 20, 10, 3  # button_width, button_height, space_between_buttons_x, space_between_buttons_y, num_columns\n",
    "    total_width = (params[0] * params[4]) + (params[2] * (params[4] - 1))\n",
    "    start_x, start_y = ((bg_image.width() - total_width) / 2), (bg_image.height() / 4)\n",
    "\n",
    "    # Closure to handle button click event and close the dialog\n",
    "    def make_selection(value):\n",
    "        result[0] = value\n",
    "        dialog.destroy()\n",
    "\n",
    "    # Dynamically create and place buttons based on the options provided\n",
    "    for index, (text, value) in enumerate(options.items()):\n",
    "        x, y = start_x + (index % params[4]) * (params[0] + params[2]), start_y + (index // params[4]) * (params[1] + params[3])\n",
    "        tk.Button(dialog, text=text, command=lambda v=value: make_selection(v), font=(\"Helvetica\", 10, \"bold\")).place(x=x, y=y, width=params[0], height=params[1])\n",
    "\n",
    "    # Wait for the user to make a selection or close the dialog\n",
    "    dialog.wait_window()\n",
    "    return result[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cdec34-89c3-473f-be6c-4a1d9c891cd3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 0.3 Capture  & Output Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbef1bd9-8e15-4af8-b1bb-001d2a104e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_capture():\n",
    "    '''\n",
    "    Initializes video capture based on user selection for analysis type and input source.\n",
    "\n",
    "    This function presents GUI dialogs for the user to select the type of posture analysis and the input source (image, video file, or camera). \n",
    "    Based on the user's selections, it sets up the video capture source and maps the appropriate analysis and detection functions.\n",
    "\n",
    "    Returns:\n",
    "    A tuple containing the video capture object, a boolean indicating if the source is an image, the path to the file (if applicable), \n",
    "    the frame rate of the video (if applicable), the analysis function, the detection function, and the analysis choice index.\n",
    "    '''\n",
    "    # Initialize a hidden tkinter window to avoid showing an empty window\n",
    "    ROOT = tk.Tk()\n",
    "    ROOT.withdraw()\n",
    "\n",
    "    # Map user choices to corresponding analysis and detection function pairs\n",
    "    ad_map = {\n",
    "        1: (front_angle_analysis, front_angle_detection), 2: (side_angle_analysis, side_angle_detection),\n",
    "        3: (balance_back_analysis, balance_back_detection), 4: (balance_test_analysis, balance_test_detection),\n",
    "        5: (balance_front_analysis, balance_front_detection), 6: (balance_side_analysis, balance_side_detection)\n",
    "    }\n",
    "\n",
    "    # Options for analysis type and input source\n",
    "    a_opts = {\"Front Angle\": 1, \"Side Angle\": 2, \"Balance Back\": 3, \"Balance Test\": 4, \"Balance Front\": 5, \"Balance Side\": 6}\n",
    "    i_opts = {\"Image\": 1, \"Video file\": 2, \"Camera\": 3}\n",
    "\n",
    "    # Get user selections using the GUI dialogs\n",
    "    a_choice = gui(a_opts) or sys.exit(0)  # Exit if no analysis type is chosen\n",
    "    anal_func, detect_func = ad_map[a_choice]\n",
    "    i_choice = gui(i_opts) or sys.exit(0)  # Exit if no input source is chosen\n",
    "\n",
    "    # Determine the file type filter based on input choice\n",
    "    file_type = [(\"Image files\", \"*.jpg *.jpeg *.png\")] if i_choice == 1 else [(\"Video files\", \"*.mp4 *.mov *.avi\")]\n",
    "    # Ask for file path if the choice is not camera\n",
    "    path = filedialog.askopenfilename(title=\"Select the file\", filetypes=file_type) if i_choice != 3 else None\n",
    "    if i_choice != 3 and not path: sys.exit(0)  # Exit if no file is selected\n",
    "\n",
    "    # Setup video capture based on user selection\n",
    "    cap = cv2.VideoCapture(0 if i_choice == 3 else path)\n",
    "    # Try setting a very high resolution\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 9999)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 9999)\n",
    "    frame_rate = cap.get(cv2.CAP_PROP_FPS) if cap.isOpened() else 0\n",
    "\n",
    "    return cap, i_choice == 1, path, frame_rate, anal_func, detect_func, a_choice-1\n",
    "\n",
    "\n",
    "def setup_output_writer(cap, is_image, timestamp):\n",
    "    '''\n",
    "    Sets up a writer object to save the processed video or image output.\n",
    "    Depending on whether the input is an image or a video, this function creates the appropriate output file path and, \n",
    "    for videos, initializes a VideoWriter object with the input video's frame rate and resolution.\n",
    "\n",
    "    Args:\n",
    "    - cap: The video capture object.\n",
    "    - is_image: A boolean indicating whether the input source is an image.\n",
    "    - timestamp: A timestamp string used to uniquely name the output file.\n",
    "\n",
    "    Returns:\n",
    "    For images, returns the output file path. For videos, returns a VideoWriter object.\n",
    "    '''\n",
    "    # Initialize a hidden tkinter window to avoid showing an empty window\n",
    "    ROOT = tk.Tk()\n",
    "    ROOT.withdraw()\n",
    "\n",
    "    # Define the output directory\n",
    "    output_folder = 'output'\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    # Define the output file path\n",
    "    out_path = f\"{output_folder}/Result_{timestamp}\" + ('.jpg' if is_image else '.mp4')\n",
    "    \n",
    "    # If the input is not an image, create and return a VideoWriter object\n",
    "    if not is_image:\n",
    "        return cv2.VideoWriter(\n",
    "            out_path, \n",
    "            cv2.VideoWriter_fourcc(*'MP4V'), \n",
    "            cap.get(cv2.CAP_PROP_FPS), \n",
    "            (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "        )\n",
    "    # For images, simply return the file path\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc62bad8-0fd0-4752-b9c9-b167c03c7a34",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 0.4 Frame Processing and Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b4f729-8b3d-45c3-8b8b-3495332c3b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(frame, pose, anal_func, detect_func):\n",
    "    '''\n",
    "    Processes a single frame to detect pose landmarks, analyze posture, and annotate the frame.\n",
    "\n",
    "    This function converts the frame to RGB, applies pose detection, then, if landmarks are found, it uses the analysis function to compute angles and the detection function to annotate the frame with the computed data.\n",
    "\n",
    "    Args:\n",
    "    - frame: The input frame from the video or camera.\n",
    "    - pose: The pose estimation model.\n",
    "    - anal_func: Function to analyze the pose and compute angles.\n",
    "    - detect_func: Function to draw annotations on the frame based on analysis.\n",
    "\n",
    "    Returns:\n",
    "    A tuple containing the annotated image and a dictionary with angles data if landmarks are detected, otherwise the original frame and an empty dictionary.\n",
    "    '''\n",
    "    # Convert frame from BGR to RGB color space for pose detection\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image)  # Apply pose detection\n",
    "    \n",
    "    # Check if any pose landmarks were detected\n",
    "    if results.pose_landmarks:\n",
    "        # Compute angles or other metrics from the pose landmarks\n",
    "        angles_data = anal_func(results.pose_landmarks.landmark, mp_pose)\n",
    "        # Annotate the frame with the results of the analysis\n",
    "        annotated_image = detect_func(cv2.cvtColor(image, cv2.COLOR_RGB2BGR), results, angles_data)\n",
    "        return annotated_image, angles_data\n",
    "    \n",
    "    # If no landmarks are detected, return the original frame (converted back to BGR) and an empty dictionary\n",
    "    return cv2.cvtColor(image, cv2.COLOR_RGB2BGR), {}\n",
    "\n",
    "def draw_colored_connection(image, results, start_idx, end_idx, color=(255, 0, 0), thickness=2):\n",
    "    '''\n",
    "    Draws a colored line between two specified landmarks on the image.\n",
    "\n",
    "    Args:\n",
    "    - image: The image on which to draw.\n",
    "    - results: The pose detection results containing landmarks.\n",
    "    - start_idx: The index of the start landmark.\n",
    "    - end_idx: The index of the end landmark.\n",
    "    - color: The color of the line (default red).\n",
    "    - thickness: The thickness of the line (default 2).\n",
    "    '''\n",
    "    # Ensure pose landmarks are present\n",
    "    if results.pose_landmarks:\n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "        start, end = landmarks[start_idx], landmarks[end_idx]\n",
    "        # Draw a line between the start and end landmarks\n",
    "        cv2.line(image, (int(start.x * image.shape[1]), int(start.y * image.shape[0])),\n",
    "                 (int(end.x * image.shape[1]), int(end.y * image.shape[0])), color, thickness)\n",
    "\n",
    "\n",
    "def draw_landmarks(image, results, landmark_indices, color=(255, 255, 255), radius=3):\n",
    "    '''\n",
    "    Draws circles on specified landmarks.\n",
    "\n",
    "    Args:\n",
    "    - image: The image on which to draw.\n",
    "    - results: The pose detection results containing landmarks.\n",
    "    - landmark_indices: Indices of the landmarks to draw.\n",
    "    - color: The color of the circles (default white).\n",
    "    - radius: The radius of the circles (default 3).\n",
    "    '''\n",
    "    # Iterate over the specified landmark indices and draw each one\n",
    "    for idx in landmark_indices:\n",
    "        # Retrieve the specific landmark point\n",
    "        landmark_point = results.pose_landmarks.landmark[idx]\n",
    "        # Calculate the position of the landmark in the image\n",
    "        pos = (int(landmark_point.x * image.shape[1]), int(landmark_point.y * image.shape[0]))\n",
    "        # Draw a circle at the landmark position\n",
    "        cv2.circle(image, pos, radius, color, -1)  # -1 fills the circle\n",
    "\n",
    "\n",
    "def draw_labeled_box(image, results, joint_landmarks, angles, padding=3, font_scale=0.35, font_thickness=1,\n",
    "                     box_color=(255, 255, 255), text_color=(139, 0, 0), edge_color=(230, 216, 173)):\n",
    "    '''\n",
    "    Draws labeled boxes with angle values near specified joints.\n",
    "\n",
    "    Args:\n",
    "    - image: The image on which to draw.\n",
    "    - results: The pose detection results containing landmarks.\n",
    "    - joint_landmarks: Indices of the joints near which to draw the labeled boxes.\n",
    "    - angles: List of angle values corresponding to the joints.\n",
    "    - padding, font_scale, font_thickness: Styling parameters for the text and box.\n",
    "    - box_color: The background color of the box.\n",
    "    - text_color: The color of the text.\n",
    "    - edge_color: The color of the box's edge.\n",
    "    '''\n",
    "    # Iterate over each joint and its corresponding angle\n",
    "    for joint_index, angle in enumerate(angles):\n",
    "        # Get the landmark for the current joint\n",
    "        joint = results.pose_landmarks.landmark[joint_landmarks[joint_index]]\n",
    "        # Prepare the text to be displayed (angle value) and its position\n",
    "        angle_text, pos = f\"{round(angle)}\", (int(joint.x * image.shape[1]) + 10, int(joint.y * image.shape[0]))\n",
    "        # Calculate the size of the text box to fit the angle text\n",
    "        text_size = cv2.getTextSize(angle_text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness)[0]\n",
    "        # Define the start and end points of the text box\n",
    "        box_start, box_end = (pos[0] - padding, pos[1] + padding), (pos[0] + text_size[0] + padding, pos[1] - text_size[1] - padding)\n",
    "        # Draw the background box for the text\n",
    "        cv2.rectangle(image, box_start, box_end, box_color, cv2.FILLED)\n",
    "        # Draw the edge of the box\n",
    "        cv2.rectangle(image, box_start, box_end, edge_color, 1)\n",
    "        # Place the angle text on top of the box\n",
    "        cv2.putText(image, angle_text, (pos[0], pos[1]), cv2.FONT_HERSHEY_SIMPLEX, font_scale, text_color, font_thickness, cv2.LINE_AA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b1f371-9c03-4ea0-81d7-51dbb238b6bc",
   "metadata": {
    "id": "82007ac2-db41-4589-b3ad-89bef11d3478",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 0.5 Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3fea99-dab5-44b4-b8fb-b207d6c4007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_point(landmarks, landmark):\n",
    "    '''\n",
    "    Retrieves the (x, y) coordinates of a specified landmark.\n",
    "\n",
    "    This utility function extracts the coordinates of a given landmark from a list of landmarks, which is useful for various calculations and annotations throughout the pose estimation process.\n",
    "\n",
    "    Args:\n",
    "    - landmarks: The list or array of landmarks from which coordinates are to be extracted.\n",
    "    - landmark: The specific landmark (usually an enum or index) for which the coordinates are requested.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple (x, y) representing the coordinates of the specified landmark.\n",
    "    '''\n",
    "    return landmarks[landmark.value].x, landmarks[landmark.value].y\n",
    "\n",
    "\n",
    "def calculate_angle(a, b, c):\n",
    "    \"\"\"\n",
    "    Calculates the angle formed by three points, with an option to reverse the direction.\n",
    "\n",
    "    This function is essential for posture analysis, where calculating the angles between joints (represented by points) is necessary to determine the posture's correctness or to identify specific postural attributes.\n",
    "\n",
    "    Args:\n",
    "    - a, b, c (tuple): Coordinates of the points forming the angle (each as an (x, y) tuple).\n",
    "\n",
    "    Returns:\n",
    "    - float: The calculated angle in degrees, normalized to the range [-180, 180].\n",
    "    \"\"\"\n",
    "    # Convert points to numpy arrays for easier mathematical operations\n",
    "    a, b, c = np.array(a), np.array(b), np.array(c)\n",
    "    \n",
    "    # Calculate the radians between the points\n",
    "    radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(a[1] - b[1], a[0] - b[0])\n",
    "    \n",
    "    # Convert radians to degrees and normalize the angle\n",
    "    angle = np.degrees(radians)\n",
    "    angle = (angle + 360) % 360\n",
    "    if angle > 180:\n",
    "        angle -= 360\n",
    "    \n",
    "    return abs(angle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7976b31e-4f3e-44a4-bd30-ccc4692465c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 0.6 Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2725821c-3aba-4c09-8cd7-8deb18fb6432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report(joint_angles_df, frame_rate, body_labels, analysis_choice, timestamp):\n",
    "    '''\n",
    "    Generates a PDF report containing plots of joint angles over time and statistical summaries.\n",
    "\n",
    "    Args:\n",
    "    - joint_angles_df (DataFrame): Contains the joint angles data with each column representing a joint angle.\n",
    "    - frame_rate (float): The frame rate of the video from which the data was extracted, used to calculate the time axis for plots.\n",
    "    - body_labels (list of lists): Contains labels for the joints or angles measured, organized by analysis type.\n",
    "    - analysis_choice (int): Indicates the chosen analysis type, affecting which joints/angles are included in the report.\n",
    "    - timestamp (str): Timestamp string used to uniquely name the output file.\n",
    "\n",
    "    The function checks if the number of detected joints matches the expected number for the chosen analysis. If there's a mismatch, it outputs a placeholder report indicating \n",
    "    \"No Posture detected.\" For valid data, it generates a report with time-series plots of each joint angle and tables summarizing average, maximum, \n",
    "    and minimum values of these angles.\n",
    "\n",
    "    The report is saved as a PDF file named \"Report_<timestamp>.pdf\" in the \"output\" directory.\n",
    "    '''\n",
    "\n",
    "    # Initial setup: Determine which joints to include based on analysis choice\n",
    "    joints = joint_angles_df.columns.tolist()\n",
    "    if analysis_choice == 0 or analysis_choice == 3:\n",
    "        joints = joints[:-2]  # Exclude certain joints for specific analysis\n",
    "    elif analysis_choice == 3:\n",
    "        joints = joints[:-1]\n",
    "\n",
    "    # Add a time column based on frame rate for plotting\n",
    "    joint_angles_df['Time'] = joint_angles_df.index / frame_rate\n",
    "\n",
    "    # Check for matching joint count to ensure correct report generation\n",
    "    if len(body_labels[analysis_choice]) != len(joints):\n",
    "        # Create a simple report if joint count doesn't match expected\n",
    "        fig = plt.figure(figsize=(10, 6))\n",
    "        plt.suptitle('Analysis Report', fontsize=20)\n",
    "        plt.text(0.5, 0.5, 'No Posture detected.', fontsize=14, ha='center')\n",
    "        plt.axis('off')\n",
    "        os.makedirs('output', exist_ok=True)  # Ensure the output directory exists\n",
    "        plt.savefig(os.path.join('output', f\"Report_{timestamp}.pdf\"))\n",
    "        plt.close(fig)\n",
    "        return  # Exit the function after handling the mismatch case\n",
    "\n",
    "    # Prepare the figure for detailed report generation\n",
    "    num_rows, fig_width, fig_height = ((len(joints) + 1) // 2) + 5, 18, (((len(joints) + 1) // 2) + 5) * 4\n",
    "    fig = plt.figure(figsize=(fig_width, fig_height))\n",
    "    fig.suptitle('Analysis Report', fontsize=20, y=1)\n",
    "    gs = GridSpec(num_rows, 4, figure=fig, width_ratios=[3, 1, 3, 1])\n",
    "\n",
    "    # Loop through each joint to plot angle data and statistical summaries\n",
    "    for i, joint in enumerate(joints):\n",
    "        row, col = i // 2, (i % 2) * 2\n",
    "        ax_plot = fig.add_subplot(gs[row, col])\n",
    "        ax_plot.plot(joint_angles_df['Time'], joint_angles_df[joint], label=joint)\n",
    "        ax_plot.set_title(body_labels[analysis_choice][i])\n",
    "        ax_plot.set_xlabel('Time (s)')\n",
    "        ax_plot.set_ylabel('Angle (degrees)')\n",
    "        ax_plot.grid(True)  # Add grid for better readability\n",
    "\n",
    "        # Calculate statistical summaries for each joint angle\n",
    "        stats = {'Average': round(np.mean(joint_angles_df[joint]), 2),\n",
    "                 'Max': round(np.max(joint_angles_df[joint]), 2),\n",
    "                 'Min': round(np.min(joint_angles_df[joint]), 2)}\n",
    "        ax_table = fig.add_subplot(gs[row, col + 1])\n",
    "        ax_table.axis('off')  # Hide axes for the table\n",
    "        table = ax_table.table(cellText=[[k, f'{v:.2f}'] for k, v in stats.items()],\n",
    "                               colLabels=['Stat', 'Value'], cellLoc='center', loc='center')\n",
    "        table.auto_set_font_size(True)\n",
    "        table.scale(1, 1.5)  # Adjust table scaling\n",
    "\n",
    "    plt.subplots_adjust(hspace=1)\n",
    "    fig.tight_layout()  # Adjust layout to make it tight and neat\n",
    "    os.makedirs('output', exist_ok=True)  # Ensure the output directory exists\n",
    "    plt.savefig(os.path.join('output', f\"Report_{timestamp}.pdf\"))  # Save the final report as PDF\n",
    "    plt.close(fig)  # Close the plot to free resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c674a9-9b5b-4740-a903-ae3bf5a753c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 0.7 Execution Flow Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859a7e2c",
   "metadata": {
    "id": "859a7e2c"
   },
   "outputs": [],
   "source": [
    "def run_estimation():\n",
    "    '''\n",
    "    Orchestrates the posture analysis process from initialization to report generation.\n",
    "\n",
    "    This function handles the setup of video capture based on user input, initiates posture analysis, and generates a report upon completion. \n",
    "    It utilizes multithreading to allow users to stop the analysis process interactively.\n",
    "\n",
    "    Global Variables:\n",
    "    - stop_evaluation: A boolean flag used to control the analysis loop, allowing the process to be stopped.\n",
    "    - joint_angles_df: A DataFrame that accumulates the angles data extracted from the video or image analysis.\n",
    "    '''\n",
    "    global stop_evaluation, joint_angles_df\n",
    "    # Reset control variables and data storage\n",
    "    stop_evaluation, joint_angles_df = False, pd.DataFrame()\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')  # Timestamp for file naming\n",
    "\n",
    "    # Initialize video capture and analysis settings\n",
    "    cap, is_image, _, frame_rate, anal_func, detect_func, analysis_choice = initialize_capture()\n",
    "    if not cap: return  # Exit if capture initialization fails\n",
    "\n",
    "    # Setup the output file writer for saving results\n",
    "    out = setup_output_writer(cap, is_image, timestamp)\n",
    "\n",
    "    # Start the stop/proceed control GUI in a separate thread for non-blocking execution\n",
    "    gui_thread = threading.Thread(target=show_stop_gui, daemon=True)\n",
    "    gui_thread.start()\n",
    "\n",
    "    # Initialize Mediapipe pose detection with configured confidence levels\n",
    "    with mp.solutions.pose.Pose(min_detection_confidence=d_conf_img if is_image else d_conf_vid, \n",
    "                                min_tracking_confidence=t_conf_img if is_image else t_conf_vid) as pose:\n",
    "        while cap.isOpened() and not stop_evaluation:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break  # Exit loop if no frame is read\n",
    "\n",
    "            # Process each frame for pose detection and annotation\n",
    "            processed_frame, angles_data = process_frame(frame, pose, anal_func, detect_func)\n",
    "            joint_angles_df = pd.concat([joint_angles_df, pd.DataFrame([angles_data])], ignore_index=True)\n",
    "            cv2.imshow('Mediapipe Feed', processed_frame)\n",
    "            \n",
    "            # Save the processed frame/image and handle exit conditions\n",
    "            if is_image: \n",
    "                cv2.imwrite(out, processed_frame)\n",
    "                break\n",
    "            elif out: \n",
    "                out.write(processed_frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "                break\n",
    "\n",
    "    # Cleanup resources and generate the final report\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    if out and not is_image: out.release()\n",
    "    gui_thread.join()  # Wait for the GUI thread to finish\n",
    "    generate_report(joint_angles_df, frame_rate, body_labels, analysis_choice, timestamp)\n",
    "\n",
    "def show_stop_gui():\n",
    "    '''\n",
    "    Creates a simple GUI with a \"Stop\" button to control the posture analysis process.\n",
    "\n",
    "    This function provides a graphical interface for the user to stop the ongoing analysis at any point. Upon clicking \"Stop\", the analysis process can be \n",
    "    halted, and the button then changes to \"Proceed\", allowing the user to finalize the analysis and proceed to report generation.\n",
    "\n",
    "    Global Variable:\n",
    "    - stop_evaluation: A boolean flag used to signal when the analysis should be stopped or continued.\n",
    "    '''\n",
    "    global stop_evaluation\n",
    "\n",
    "    def on_button_click():\n",
    "        # Toggle the stop_evaluation flag and update button text based on user interaction\n",
    "        global stop_evaluation\n",
    "        if button['text'] == 'Stop':\n",
    "            stop_evaluation = True\n",
    "            button.config(text='Proceed')\n",
    "        else:\n",
    "            root.destroy()\n",
    "\n",
    "    # Initialize and configure the control GUI window\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Analysis Control\")\n",
    "    tk.Label(root, text=\"Click 'Stop' to stop analysis \\nClick 'Proceed' to complete analysis.\").pack(pady=20)\n",
    "\n",
    "    button = tk.Button(root, text=\"Stop\", command=on_button_click)\n",
    "    button.pack(pady=10)\n",
    "\n",
    "    # Handle window close event to ensure proper resource cleanup\n",
    "    root.protocol(\"WM_DELETE_WINDOW\", root.destroy)\n",
    "    root.mainloop()\n",
    "\n",
    "    return stop_evaluation\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    The main function repeatedly runs the posture estimation process until manually stopped.\n",
    "\n",
    "    This loop allows continuous operation, processing multiple images or videos and generating reports for each until the program is explicitly terminated.\n",
    "    '''\n",
    "    while run:\n",
    "        run_estimation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8f6f4c",
   "metadata": {
    "id": "fb8f6f4c",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1.1 Front Angle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707b7fd9",
   "metadata": {
    "id": "707b7fd9"
   },
   "source": [
    "## 1.1.1 Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb2da94",
   "metadata": {
    "id": "3fb2da94"
   },
   "outputs": [],
   "source": [
    "def front_angle_analysis(landmarks, mp_pose):\n",
    "    \"\"\"\n",
    "    Analyzes front-view squat posture based on landmarks.\n",
    "\n",
    "    Args:\n",
    "    - landmarks (list): List of detected pose landmarks.\n",
    "    - mp_pose (module): Mediapipe pose module for landmark references.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Returns a tuple containing a tuple of angles (left knee angle, right knee angle, hip angle difference).\n",
    "    \"\"\"\n",
    "\n",
    "    # Get points\n",
    "    left_shoulder, left_elbow, left_wrist, left_index, left_hip, left_knee, left_ankle, left_foot_index = map(lambda lm: get_point(landmarks, lm),\n",
    "                                                                                                  [landmarks_to_draw[11], landmarks_to_draw[13], landmarks_to_draw[15],\n",
    "                                                                                                  landmarks_to_draw[19], landmarks_to_draw[23], landmarks_to_draw[25], \n",
    "                                                                                                  landmarks_to_draw[27], landmarks_to_draw[31]])\n",
    "    right_shoulder, right_elbow, right_wrist, right_index, right_hip, right_knee, right_ankle, right_foot_index = map(lambda lm: get_point(landmarks, lm),\n",
    "                                                                                                  [landmarks_to_draw[12], landmarks_to_draw[14], landmarks_to_draw[16],\n",
    "                                                                                                  landmarks_to_draw[20], landmarks_to_draw[24], landmarks_to_draw[26], \n",
    "                                                                                                  landmarks_to_draw[28], landmarks_to_draw[32]])\n",
    "\n",
    "\n",
    "    # Calculate shoulder angles\n",
    "    left_shoulder_angle = calculate_angle(left_hip, left_shoulder, left_elbow)\n",
    "    right_shoulder_angle = calculate_angle(right_hip, right_shoulder, right_elbow)\n",
    "\n",
    "    # Calculate elbow angles\n",
    "    left_elbow_angle = calculate_angle(left_shoulder, left_elbow, left_wrist)\n",
    "    right_elbow_angle = calculate_angle(right_shoulder, right_elbow, right_wrist)\n",
    "\n",
    "    # Calculate wrist angles\n",
    "    left_wrist_angle = calculate_angle(left_index, left_wrist, left_elbow)\n",
    "    right_wrist_angle = calculate_angle(right_index, right_wrist, right_elbow)\n",
    "\n",
    "    # Calculate hip angles\n",
    "    left_hip_angle = calculate_angle(left_shoulder, left_hip, left_knee)\n",
    "    right_hip_angle = calculate_angle(right_shoulder, right_hip, right_knee)\n",
    "\n",
    "    # Calculate knee angles\n",
    "    left_knee_angle = calculate_angle(left_hip, left_knee, left_ankle)\n",
    "    right_knee_angle = calculate_angle(right_hip, right_knee, right_ankle)\n",
    "\n",
    "    # Calculate ankle angles\n",
    "    left_ankle_angle = calculate_angle(left_ankle, left_foot_index, left_knee)\n",
    "    right_ankle_angle = calculate_angle(right_ankle, right_foot_index, right_knee)\n",
    "\n",
    "    # Calculate the midpoint of the shoulders\n",
    "    shoulder_mid_x = (left_shoulder[0] + right_shoulder[0]) / 2\n",
    "    \n",
    "    # Calculate the central vertical line (midpoint between hips)\n",
    "    central_vertical_line_x = (left_hip[0] + right_hip[0]) / 2\n",
    "\n",
    "    shoulder_dev = (shoulder_mid_x - central_vertical_line_x) / abs(left_shoulder[0] - right_shoulder[0])\n",
    "    # Return angles as a tuple\n",
    "    return (left_shoulder_angle, right_shoulder_angle, left_elbow_angle, right_elbow_angle, left_wrist_angle,\n",
    "                     right_wrist_angle, left_hip_angle, right_hip_angle, (left_knee_angle), (right_knee_angle),\n",
    "                     left_ankle_angle, right_ankle_angle, shoulder_dev, shoulder_mid_x, central_vertical_line_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5431f3",
   "metadata": {
    "id": "3c5431f3"
   },
   "source": [
    "## 1.1.2 Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7147ee",
   "metadata": {
    "id": "fd7147ee"
   },
   "outputs": [],
   "source": [
    "def front_angle_detection(image, results, angles):\n",
    "    \"\"\"\n",
    "    Analyzes and annotates a front-view image of a squatting posture.\n",
    "\n",
    "    Args:\n",
    "    - image (numpy.ndarray): The image on which to perform the analysis and annotations.\n",
    "    - results (object): The detected pose landmarks from a pose estimation model.\n",
    "    - angles (tuple): A tuple containing the angles (left knee angle, right knee angle, hip angle difference).\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The annotated image with landmarks, angles, and posture information.\n",
    "    \"\"\"\n",
    "    # Unpack angles to get the knee angles and hip angle difference\n",
    "    if results.pose_landmarks:\n",
    "\n",
    "\n",
    "\n",
    "        # Draw the lines\n",
    "        connect_idx = [(23, 25), (25, 31), (27, 31), (24, 26), (26, 32), (28, 32), (11, 13), (13, 15),\n",
    "                        (12, 14), (14, 16), (11, 23), (12, 24), (11, 12), (23, 24)]\n",
    "        colors = [(255, 0, 0)] * 12 + [(0, 0, 255)] * 4\n",
    "        for (p1, p2), color in zip(connect_idx, colors):\n",
    "            draw_colored_connection(image, results, landmarks_to_draw[p1], landmarks_to_draw[p2], color=color)\n",
    "\n",
    "        # Draw the specified landmarks\n",
    "        landmark_idx = [11, 12, 13, 14, 15, 16, 23, 24, 25, 26, 31, 32, 27, 28]\n",
    "        draw_landmarks(image, results, landmark_idx)\n",
    "        joint_landmarks = [landmarks_to_draw[idx].value for idx in landmark_idx] # Draw the lines and display angles\n",
    "        draw_labeled_box(image, results, joint_landmarks, angles[:-3]) # Call the draw_labeled_box function\n",
    "\n",
    "        # central line\n",
    "        \n",
    "        central_vertical_line_x, shoulder_mid_x = angles[-1], angles[-2]\n",
    "        # Draw central vertical line\n",
    "        central_vertical_line_pixel_x = int(central_vertical_line_x * image.shape[1])\n",
    "        cv2.line(image, (central_vertical_line_pixel_x, 0), (central_vertical_line_pixel_x, image.shape[0]), (255, 255, 0), 1)\n",
    "\n",
    "        # Calculate middle dot position (midpoint between shoulders)\n",
    "        middle_dot_x = int(shoulder_mid_x * image.shape[1])\n",
    "        \n",
    "        middle_dot_y = int((results.pose_landmarks.landmark[landmarks_to_draw[landmark_idx[0]]].y +\n",
    "                            results.pose_landmarks.landmark[landmarks_to_draw[landmark_idx[1]]].y) / 2 * image.shape[0])\n",
    "\n",
    "        # Draw the middle dot\n",
    "        cv2.circle(image, (middle_dot_x, middle_dot_y), 3, (0, 255, 0), -1)  # Green dot\n",
    "        shoulder_midpoint_pos = (middle_dot_x, middle_dot_y)\n",
    "        # For \"Shoulder Diff\" text\n",
    "        deviation_text = 'Left' if (shoulder_mid_x - central_vertical_line_x) > 0 else 'Right' if (shoulder_mid_x - central_vertical_line_x) < 0 else 'Centered'\n",
    "        deviation_full_text = f'Dev: {deviation_text}'\n",
    "\n",
    "        # Calculate size of the text for background box calculation\n",
    "        deviation_text_size = cv2.getTextSize(deviation_full_text, cv2.FONT_HERSHEY_COMPLEX , 0.4, 2)[0]\n",
    "        deviation_box_start = (central_vertical_line_pixel_x + 5, shoulder_midpoint_pos[1] + 15 - deviation_text_size[1] - 2)\n",
    "        deviation_box_end = (deviation_box_start[0] + deviation_text_size[0] + 4, shoulder_midpoint_pos[1] + 15 + 2)\n",
    "        cv2.rectangle(image, deviation_box_start, deviation_box_end, (255, 255, 255), cv2.FILLED)\n",
    "        cv2.rectangle(image, deviation_box_start, deviation_box_end, (230, 216, 173), 1)\n",
    "        # put the text on top of the boxes\n",
    "        cv2.putText(image, deviation_full_text, (deviation_box_start[0], deviation_box_start[1] + deviation_text_size[1] + 2), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (139, 0, 0), 1)\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0df6385",
   "metadata": {
    "id": "a0df6385",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1.2 Side Angle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26487618",
   "metadata": {
    "id": "26487618"
   },
   "source": [
    "## 1.2.1 Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f8dbde",
   "metadata": {
    "id": "17f8dbde"
   },
   "outputs": [],
   "source": [
    "def side_angle_analysis(landmarks, mp_pose):\n",
    "    \"\"\"\n",
    "    Analyzes front-view squat posture based on landmarks.\n",
    "\n",
    "    Args:\n",
    "    - landmarks (list): List of detected pose landmarks.\n",
    "    - mp_pose (module): Mediapipe pose module for landmark references.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Returns a tuple containing the detected posture and a tuple of angles (left knee angle, right knee angle, hip angle difference).\n",
    "    \"\"\"\n",
    "    # Get points\n",
    "    left_shoulder, left_elbow, left_wrist, left_hip, left_knee, left_ankle, left_foot_index, left_ear = map(lambda lm: get_point(landmarks, lm),\n",
    "                                                                                                  [landmarks_to_draw[11], landmarks_to_draw[13], landmarks_to_draw[15],\n",
    "                                                                                                  landmarks_to_draw[23], landmarks_to_draw[25], landmarks_to_draw[27],\n",
    "                                                                                                  landmarks_to_draw[31], landmarks_to_draw[7]])\n",
    "    right_shoulder, right_elbow, right_wrist, right_hip, right_knee, right_ankle, right_foot_index, right_ear = map(lambda lm: get_point(landmarks, lm),\n",
    "                                                                                                  [landmarks_to_draw[12], landmarks_to_draw[14], landmarks_to_draw[16],\n",
    "                                                                                                  landmarks_to_draw[24], landmarks_to_draw[26], landmarks_to_draw[28],\n",
    "                                                                                                  landmarks_to_draw[32], landmarks_to_draw[8]])\n",
    "\n",
    "    # Analyse hip posture\n",
    "    # Calculate hip angles\n",
    "    left_hip_angle = calculate_angle(left_shoulder, left_hip, [left_hip[0], 0])\n",
    "    right_hip_angle = calculate_angle(right_shoulder, right_hip, [right_hip[0], 0])\n",
    "\n",
    "    # Calculate elbow angles\n",
    "    left_elbow_angle = calculate_angle(left_shoulder, left_elbow, left_wrist)\n",
    "    right_elbow_angle = calculate_angle(right_shoulder, right_elbow, right_wrist)\n",
    "\n",
    "    # Calculate shoulder angles\n",
    "    left_shoulder_angle = calculate_angle(left_elbow, left_shoulder, left_hip)\n",
    "    right_shoulder_angle = calculate_angle(right_elbow, right_shoulder, right_hip)\n",
    "\n",
    "    # Calculate torso angles\n",
    "    left_torso_angle = calculate_angle(left_shoulder, left_hip, left_knee)\n",
    "    right_torso_angle = calculate_angle(right_shoulder, left_hip, right_knee)\n",
    "\n",
    "    # Calculate ankle front angles\n",
    "    left_ankle_f_angle = calculate_angle(left_knee, left_ankle, left_foot_index)\n",
    "    right_ankle_f_angle = calculate_angle(right_knee, right_ankle, right_foot_index)\n",
    "\n",
    "     # Calculate neck angle\n",
    "    left_neck_angle = calculate_angle([left_shoulder[0], 0], left_shoulder, left_ear)\n",
    "    right_neck_angle = calculate_angle([right_shoulder[0], 0], right_shoulder, right_ear)\n",
    "\n",
    "    # Analyse ankle posture\n",
    "    # Calculate ankle angles\n",
    "    left_ankle_angle = calculate_angle(left_knee, left_ankle, [left_ankle[0], 0])\n",
    "    right_ankle_angle = calculate_angle(right_knee, right_ankle, [right_ankle[0], 0])\n",
    "\n",
    "    # Analyse knee posture\n",
    "    # Calculate knee angles, subtract 180 for reflection\n",
    "    left_knee_angle = calculate_angle(left_hip, left_knee, left_ankle)\n",
    "    right_knee_angle = calculate_angle(right_hip, right_knee, right_ankle)\n",
    "\n",
    "    return (left_shoulder_angle, right_shoulder_angle, left_elbow_angle, right_elbow_angle,\n",
    "                     left_hip_angle, right_hip_angle, left_knee_angle, right_knee_angle, left_ankle_angle,\n",
    "                     right_ankle_angle, left_neck_angle, right_neck_angle, left_torso_angle, right_torso_angle,\n",
    "                     left_ankle_f_angle, right_ankle_f_angle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bcf3c7",
   "metadata": {
    "id": "53bcf3c7"
   },
   "source": [
    "## 1.2.2 Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a1df32",
   "metadata": {
    "id": "58a1df32"
   },
   "outputs": [],
   "source": [
    "def side_angle_detection(image, results, angles):\n",
    "    \"\"\"\n",
    "    Analyzes and annotates a side-view image of a squatting posture.\n",
    "\n",
    "    Args:\n",
    "    - image (numpy.ndarray): The image on which to perform the analysis and annotations.\n",
    "    - results (object): The detected pose landmarks from a pose estimation model.\n",
    "    - angles (list): A list of knee angles.\n",
    "    - body_tilts (list): List of body tilts (left and right hip and ankle angles).\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The annotated image with landmarks and angles.\n",
    "    \"\"\"\n",
    "    if results.pose_landmarks:\n",
    "\n",
    "\n",
    "        # Draw the lines with specified colors\n",
    "        connect_idx = [(7, 11), (8, 12), (11, 23), (25, 27), (12, 24), (26, 28), (11, 13), (13, 15),\n",
    "               (12, 14), (14, 16), (23, 25), (27, 31), (24, 26), (28, 32)]\n",
    "\n",
    "        for i, (p1, p2) in enumerate(connect_idx):\n",
    "            color = (0,165,255) if i < 2 else ((0, 0, 255) if i < 6 else (255, 0, 0))\n",
    "            draw_colored_connection(image, results, landmarks_to_draw[p1], landmarks_to_draw[p2], color=color)\n",
    "\n",
    "        # Draw the specified landmarks\n",
    "        landmark_idx = [11, 12, 13, 14, 23, 24, 25, 26, 27, 28, 7, 8, 15, 16, 31, 32]\n",
    "        draw_landmarks(image, results, landmark_idx)\n",
    "        joint_landmarks = [landmarks_to_draw[idx].value for idx in landmark_idx]\n",
    "        draw_labeled_box(image, results, joint_landmarks, angles[:-4]) # Call the draw_labeled_box function\n",
    "\n",
    "\n",
    "\n",
    "        # Find pose direction\n",
    "        left_foot_index_x, right_foot_index_x = results.pose_landmarks.landmark[31].x, results.pose_landmarks.landmark[32].x\n",
    "        left_ankle_x, right_ankle_x = results.pose_landmarks.landmark[27].x, results.pose_landmarks.landmark[28].x\n",
    "        if left_foot_index_x < left_ankle_x or right_foot_index_x < right_ankle_x:\n",
    "            direction = 1\n",
    "        else:\n",
    "            direction = -1\n",
    "\n",
    "\n",
    "\n",
    "        # Draw box and text for torsos and angles\n",
    "        landmark_idx_extra = [23, 24, 27, 28]\n",
    "        joint_names_extra = ['L Torso', 'R Torso', 'L Ankle', 'R Ankle']\n",
    "        joint_landmarks_extra = [landmarks_to_draw[idx].value for idx in landmark_idx_extra]\n",
    "        for joint_index, angle in enumerate(angles[-4:]):\n",
    "            joint_landmark = results.pose_landmarks.landmark[joint_landmarks_extra[joint_index]]\n",
    "\n",
    "            angle_text = f\"{round(angle)}\"\n",
    "            text_x = int(joint_landmark.x * image.shape[1]) - (90 * direction)\n",
    "            text_y = int(joint_landmark.y * image.shape[0])\n",
    "\n",
    "            # Determine the size of the text box\n",
    "            text_size, _ = cv2.getTextSize(angle_text, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)\n",
    "            text_width, text_height = text_size\n",
    "\n",
    "            box_start = (text_x - 2, text_y + 2)\n",
    "            box_end = (text_x + text_width + 2, text_y - text_height - 2)\n",
    "\n",
    "            # Draw the filled rectangle (background)\n",
    "            cv2.rectangle(image, box_start, box_end, (255, 255, 255), cv2.FILLED)\n",
    "            # Draw the border rectangle (edges)\n",
    "            cv2.rectangle(image, box_start, box_end, (230, 216, 173), 1)\n",
    "\n",
    "            # Now put the text (in specified text color)\n",
    "            text_org = (text_x, text_y)\n",
    "            cv2.putText(image, angle_text, text_org, cv2.FONT_HERSHEY_SIMPLEX, 0.35, (139, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f7602b",
   "metadata": {
    "id": "c1f7602b",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1.3 Balance Back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065b7108",
   "metadata": {
    "id": "065b7108"
   },
   "source": [
    "## 1.3.1 Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacb512a",
   "metadata": {
    "id": "eacb512a"
   },
   "outputs": [],
   "source": [
    "def balance_back_analysis(landmarks, mp_pose):\n",
    "    \"\"\"\n",
    "    Analyzes back-view squat posture based on landmarks.\n",
    "\n",
    "    Args:\n",
    "    - landmarks (list): List of detected pose landmarks.\n",
    "    - mp_pose (module): Mediapipe pose module for landmark references.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Returns a tuple containing a tuple of angles (shoulder angle difference, hip angle difference).\n",
    "    \"\"\"\n",
    "\n",
    "    # Get points\n",
    "    left_shoulder, left_hip, left_elbow = map(lambda lm: get_point(landmarks, lm), [pose_landmark.LEFT_SHOULDER, pose_landmark.LEFT_HIP, pose_landmark.LEFT_ELBOW])\n",
    "    right_shoulder, right_hip, right_elbow = map(lambda lm: get_point(landmarks, lm), [pose_landmark.RIGHT_SHOULDER, pose_landmark.RIGHT_HIP, pose_landmark.RIGHT_ELBOW])\n",
    "\n",
    "    # Posture Analysis\n",
    "    # Calculate shoulder angles\n",
    "    left_shoulder_angle = calculate_angle(right_shoulder, left_shoulder, [left_shoulder[0], 0])\n",
    "    right_shoulder_angle = calculate_angle(left_shoulder, right_shoulder, [right_shoulder[0], 0])\n",
    "\n",
    "\n",
    "     # Calculate hip angles\n",
    "    left_hip_angle = calculate_angle(right_hip, left_hip, [left_hip[0], 0])\n",
    "    right_hip_angle = calculate_angle(left_hip, right_hip, [right_hip[0], 0])\n",
    "\n",
    "    # Calculate elbow angles\n",
    "    left_elbow_angle = calculate_angle(right_elbow, left_elbow, [left_elbow[0], 0])\n",
    "    right_elbow_angle = calculate_angle(left_elbow, right_elbow, [right_elbow[0], 0])\n",
    "\n",
    "    return (left_shoulder_angle - right_shoulder_angle, left_elbow_angle - right_elbow_angle, left_hip_angle - right_hip_angle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41587520",
   "metadata": {
    "id": "41587520"
   },
   "source": [
    "## 1.3.2 Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05630747",
   "metadata": {
    "id": "05630747"
   },
   "outputs": [],
   "source": [
    "def balance_back_detection(image, results, angles):\n",
    "    if results.pose_landmarks:\n",
    "        landmark_idx = [11, 12, 13, 14, 23, 24]\n",
    "        # Draw the lines\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[landmark_idx[0]], landmarks_to_draw[landmark_idx[1]])\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[landmark_idx[2]], landmarks_to_draw[landmark_idx[3]])\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[landmark_idx[4]], landmarks_to_draw[landmark_idx[5]])\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[landmark_idx[0]], landmarks_to_draw[landmark_idx[4]], color=(0, 0, 255))\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[landmark_idx[1]], landmarks_to_draw[landmark_idx[5]], color=(0, 0, 255))\n",
    "\n",
    "        draw_landmarks(image, results, landmark_idx) # Draw the specified landmarks\n",
    "\n",
    "        joint_landmarks = [landmarks_to_draw[idx].value for idx in landmark_idx[::2]] # Draw the lines and display angles\n",
    "        draw_labeled_box(image, results, joint_landmarks, angles) # Call the draw_labeled_box function\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7338b0b",
   "metadata": {
    "id": "b7338b0b",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2. Balance Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19529e1a",
   "metadata": {
    "id": "19529e1a",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.0.1 Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3967ce5",
   "metadata": {
    "id": "c3967ce5"
   },
   "outputs": [],
   "source": [
    "def balance_test_analysis(landmarks, mp_pose):\n",
    "    \"\"\"\n",
    "    Analyzes posture during knee-raising exercises based on landmarks.\n",
    "\n",
    "    Args:\n",
    "    - landmarks (list): List of detected pose landmarks.\n",
    "    - mp_pose (module): Mediapipe pose module for landmark references.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Returns a tuple containing a list with shoulder angle difference, shoulder midpoint x-coordinate, and central vertical line x-coordinate.\n",
    "    \"\"\"\n",
    "    # Get points for shoulders and hips\n",
    "    left_shoulder, left_hip, left_heel = map(lambda lm: get_point(landmarks, lm), [pose_landmark.LEFT_SHOULDER, pose_landmark.LEFT_HIP, pose_landmark.LEFT_HEEL])\n",
    "    right_shoulder, right_hip, right_heel = map(lambda lm: get_point(landmarks, lm), [pose_landmark.RIGHT_SHOULDER, pose_landmark.RIGHT_HIP, pose_landmark.RIGHT_HEEL])\n",
    "\n",
    "    # Posture Analysis\n",
    "    # Calculate shoulder angles\n",
    "    left_shoulder_angle = calculate_angle(right_shoulder, left_shoulder, [left_shoulder[0], 0])\n",
    "    right_shoulder_angle = calculate_angle(left_shoulder, right_shoulder, [right_shoulder[0], 0])\n",
    "\n",
    "     # Calculate the midpoints\n",
    "    shoulder_mid_x = (left_shoulder[0] + right_shoulder[0]) / 2\n",
    "    hip_mid_x = (left_hip[0] + right_hip[0]) / 2\n",
    "    \n",
    "    # Calculate the central vertical line (midpoint between heels)\n",
    "    central_vertical_line_x = (left_heel[0] + right_heel[0]) / 2\n",
    "    shoulder_dev = (shoulder_mid_x - central_vertical_line_x) / abs(left_shoulder[0] - right_shoulder[0])\n",
    "    hip_dev = (hip_mid_x - central_vertical_line_x) / abs(left_hip[0] - right_hip[0])\n",
    "\n",
    "    return (shoulder_dev, hip_dev, central_vertical_line_x, shoulder_mid_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bf3764",
   "metadata": {
    "id": "57bf3764"
   },
   "source": [
    "## 2.0.2 Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda23ce1",
   "metadata": {
    "id": "fda23ce1"
   },
   "outputs": [],
   "source": [
    "def balance_test_detection(image, results, angles):\n",
    "    \"\"\"\n",
    "    Analyzes and annotates an image for knee raising exercises.\n",
    "\n",
    "    Args:\n",
    "    - image (numpy.ndarray): The image on which to perform the analysis and annotations.\n",
    "    - results (object): The detected pose landmarks from a pose estimation model.\n",
    "    - angles (tuple): Contains analysis data like shoulder vertical difference,\n",
    "                                shoulder midpoint, and central vertical line position.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The annotated image with landmarks, vertical line, and deviation information.\n",
    "    \"\"\"\n",
    "\n",
    "    should_dev, hip_dev, central_vertical_line_x, shoulder_mid_x = angles\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        landmark_idx = [11, 12, 23, 24]\n",
    "        # Draw the lines\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[landmark_idx[0]], landmarks_to_draw[landmark_idx[1]])\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[landmark_idx[2]], landmarks_to_draw[landmark_idx[3]], color=(0, 0, 255))\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[landmark_idx[0]], landmarks_to_draw[landmark_idx[2]], color=(0, 0, 255))\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[landmark_idx[1]], landmarks_to_draw[landmark_idx[3]], color=(0, 0, 255))\n",
    "\n",
    "\n",
    "        # Draw central vertical line\n",
    "        # shoulder_vertical_difference, central_vertical_line_x, shoulder_mid_x = angles[-1], angles[-2]\n",
    "        # Draw central vertical line\n",
    "        central_vertical_line_pixel_x = int(central_vertical_line_x * image.shape[1])\n",
    "        cv2.line(image, (central_vertical_line_pixel_x, 0), (central_vertical_line_pixel_x, image.shape[0]), (255, 255, 0), 1)\n",
    "        # Calculate middle dot position (midpoint between shoulders)\n",
    "        middle_dot_x = int(shoulder_mid_x * image.shape[1])\n",
    "        middle_dot_y = int((results.pose_landmarks.landmark[landmarks_to_draw[landmark_idx[0]]].y +\n",
    "                            results.pose_landmarks.landmark[landmarks_to_draw[landmark_idx[1]]].y) / 2 * image.shape[0])\n",
    "        draw_landmarks(image, results, landmark_idx)\n",
    "        # Draw the middle dot\n",
    "        cv2.circle(image, (middle_dot_x, middle_dot_y), 3, (0, 255, 0), -1)  # Green dot\n",
    "\n",
    "        # Calculate positions for drawing text\n",
    "        shoulder_midpoint_pos = (middle_dot_x, middle_dot_y)\n",
    "\n",
    "        # For \"Shoulder Diff\" text\n",
    "        shoulder_diff_text = f'Ratio: {round((should_dev), 3)}'\n",
    "        deviation_text = 'Left' if should_dev > 0 else 'Right' if should_dev < 0 else 'Centered'\n",
    "        deviation_full_text = f'Dev: {deviation_text}'\n",
    "\n",
    "        # Calculate size of the text for background box calculation\n",
    "        shoulder_diff_text_size = cv2.getTextSize(shoulder_diff_text, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)[0]\n",
    "        deviation_text_size = cv2.getTextSize(deviation_full_text, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)[0]\n",
    "\n",
    "        # Shoulder Diff Box\n",
    "        shoulder_diff_box_start = (shoulder_midpoint_pos[0] + 5, shoulder_midpoint_pos[1] - 5 - shoulder_diff_text_size[1] - 2)\n",
    "        shoulder_diff_box_end = (shoulder_diff_box_start[0] + shoulder_diff_text_size[0] + 4, shoulder_midpoint_pos[1] - 5 + 2)\n",
    "        cv2.rectangle(image, shoulder_diff_box_start, shoulder_diff_box_end, (255, 255, 255), cv2.FILLED)\n",
    "        cv2.rectangle(image, shoulder_diff_box_start, shoulder_diff_box_end, (230, 216, 173), 1)\n",
    "\n",
    "        # Deviation Box\n",
    "        deviation_box_start = (central_vertical_line_pixel_x + 5, shoulder_midpoint_pos[1] + 15 - deviation_text_size[1] - 2)\n",
    "        deviation_box_end = (deviation_box_start[0] + deviation_text_size[0] + 4, shoulder_midpoint_pos[1] + 15 + 2)\n",
    "        cv2.rectangle(image, deviation_box_start, deviation_box_end, (255, 255, 255), cv2.FILLED)\n",
    "        cv2.rectangle(image, deviation_box_start, deviation_box_end, (230, 216, 173), 1)\n",
    "\n",
    "        # Now put the text on top of the boxes\n",
    "        cv2.putText(image, shoulder_diff_text, (shoulder_diff_box_start[0], shoulder_diff_box_start[1] + shoulder_diff_text_size[1] + 2), cv2.FONT_HERSHEY_SIMPLEX, 0.35, (139, 0, 0), 1)\n",
    "        cv2.putText(image, deviation_full_text, (deviation_box_start[0], deviation_box_start[1] + deviation_text_size[1] + 2), cv2.FONT_HERSHEY_SIMPLEX, 0.35, (139, 0, 0), 1)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c73bd4",
   "metadata": {
    "id": "20c73bd4",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3.1 Balance Front"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968c33d3",
   "metadata": {
    "id": "968c33d3"
   },
   "source": [
    "## 3.1.1 Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27ca8f5",
   "metadata": {
    "id": "d27ca8f5"
   },
   "outputs": [],
   "source": [
    "def balance_front_analysis(landmarks, mp_pose):\n",
    "    \"\"\"\n",
    "    Analyzes front-view standing posture based on landmarks.\n",
    "\n",
    "    Args:\n",
    "    - landmarks (list): List of detected pose landmarks.\n",
    "    - mp_pose (module): Mediapipe pose module for landmark references.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Returns a tuple containing a tuple of angle differences for ears, shoulders, hips, knees, and ankles.\n",
    "    \"\"\"\n",
    "    # Get points for ears, shoulders, hips, knees, and ankles\n",
    "    left_ear, left_shoulder, left_hip, left_knee, left_ankle = map(\n",
    "        lambda lm: get_point(landmarks, lm),\n",
    "        [mp_pose.PoseLandmark.LEFT_EAR, mp_pose.PoseLandmark.LEFT_SHOULDER,\n",
    "         mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.LEFT_KNEE,\n",
    "         mp_pose.PoseLandmark.LEFT_ANKLE])\n",
    "\n",
    "    right_ear, right_shoulder, right_hip, right_knee, right_ankle = map(\n",
    "        lambda lm: get_point(landmarks, lm),\n",
    "        [mp_pose.PoseLandmark.RIGHT_EAR, mp_pose.PoseLandmark.RIGHT_SHOULDER,\n",
    "         mp_pose.PoseLandmark.RIGHT_HIP, mp_pose.PoseLandmark.RIGHT_KNEE,\n",
    "         mp_pose.PoseLandmark.RIGHT_ANKLE])\n",
    "\n",
    "    # Calculate joint angles\n",
    "    left_ear_angle = calculate_angle([left_ear[0], 0], left_ear, right_ear)\n",
    "    right_ear_angle = calculate_angle([right_ear[0], 0], right_ear, left_ear)\n",
    "\n",
    "    left_shoulder_angle = calculate_angle([left_shoulder[0], 0], left_shoulder, right_shoulder)\n",
    "    right_shoulder_angle = calculate_angle([right_shoulder[0], 0], right_shoulder, left_shoulder)\n",
    "\n",
    "    left_hip_angle = calculate_angle([left_hip[0], 0], left_hip, right_hip)\n",
    "    right_hip_angle = calculate_angle([right_hip[0], 0], right_hip, left_hip)\n",
    "\n",
    "    left_knee_angle = calculate_angle([left_knee[0], 0], left_knee, right_knee)\n",
    "    right_knee_angle = calculate_angle([right_knee[0], 0], right_knee, left_knee)\n",
    "\n",
    "    left_ankle_angle = calculate_angle([left_ankle[0], 0], left_ankle, right_ankle)\n",
    "    right_ankle_angle = calculate_angle([right_ankle[0], 0], right_ankle, left_ankle)\n",
    "\n",
    "    return (left_ear_angle - right_ear_angle, left_shoulder_angle - right_shoulder_angle, left_hip_angle - right_hip_angle, left_knee_angle - right_knee_angle, left_ankle_angle - right_ankle_angle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735b3cac",
   "metadata": {
    "id": "735b3cac"
   },
   "source": [
    "## 3.1.2 Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb80d84",
   "metadata": {
    "id": "afb80d84"
   },
   "outputs": [],
   "source": [
    "def balance_front_detection(image, results, angles):\n",
    "    \"\"\"\n",
    "    Analyzes and annotates an image for stand front exercises.\n",
    "\n",
    "    Args:\n",
    "    - image (numpy.ndarray): The image on which to perform the analysis and annotations.\n",
    "    - results (object): The detected pose landmarks from a pose estimation model.\n",
    "    - angles (tuple): Contains analysis data like shoulder vertical difference,\n",
    "                                shoulder midpoint, and central vertical line position.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The annotated image with landmarks, vertical line, and deviation information.\n",
    "    \"\"\"\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        landmark_idx = [7, 8, 11, 12, 23, 24, 25, 26, 27, 28]\n",
    "        # Draw the lines\n",
    "        for i in range(0, len(landmark_idx), 2):\n",
    "            draw_colored_connection(image, results, landmarks_to_draw[landmark_idx[i]], landmarks_to_draw[landmark_idx[i+1]])\n",
    "        for i in range(2, len(landmark_idx)-2, 2):\n",
    "            draw_colored_connection(image, results, landmarks_to_draw[landmark_idx[i]], landmarks_to_draw[landmark_idx[i+2]], color=(0, 0, 255))\n",
    "        for i in range(3, len(landmark_idx)-2, 2):\n",
    "            draw_colored_connection(image, results, landmarks_to_draw[landmark_idx[i]], landmarks_to_draw[landmark_idx[i+2]], color=(0, 0, 255))\n",
    "\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[landmark_idx[0]], landmarks_to_draw[landmark_idx[2]], color=(0, 0, 255))\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[landmark_idx[1]], landmarks_to_draw[landmark_idx[3]], color=(0, 0, 255))\n",
    "        draw_landmarks(image, results, landmark_idx) # Draw the specified landmarks\n",
    "\n",
    "        joint_landmarks = [landmarks_to_draw[idx].value for idx in landmark_idx[::2]] # Draw the lines and display angles\n",
    "        draw_labeled_box(image, results, joint_landmarks, angles) # Call the draw_labeled_box function\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1866ff6c",
   "metadata": {
    "id": "1866ff6c",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3.2 Balance Side"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636d94c0",
   "metadata": {
    "id": "636d94c0",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4081dd8",
   "metadata": {
    "id": "c4081dd8"
   },
   "outputs": [],
   "source": [
    "def balance_side_analysis(landmarks, mp_pose):\n",
    "    \"\"\"\n",
    "    Analyzes side-view standing posture based on landmarks.\n",
    "\n",
    "    Args:\n",
    "    - landmarks (list): List of detected pose landmarks.\n",
    "    - mp_pose (module): Mediapipe pose module for landmark references.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Returns a tuple containing the detected posture and the left shoulder angle.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get points for ear and shoulder\n",
    "    left_ear, left_shoulder, left_hip, left_knee, left_ankle = map(\n",
    "        lambda lm: get_point(landmarks, lm),\n",
    "        [mp_pose.PoseLandmark.LEFT_EAR, mp_pose.PoseLandmark.LEFT_SHOULDER,\n",
    "         mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.LEFT_KNEE, mp_pose.PoseLandmark.LEFT_ANKLE])\n",
    "\n",
    "    # Posture Analysis\n",
    "    # Calculate shoulder angle\n",
    "    left_shoulder_angle = calculate_angle([left_shoulder[0], 0], left_shoulder, left_ear)\n",
    "    left_knee_angle = calculate_angle(left_hip, left_knee, left_ankle)\n",
    "\n",
    "    return (left_shoulder_angle, left_knee_angle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a3ca1d",
   "metadata": {
    "id": "20a3ca1d"
   },
   "source": [
    "## 3.2.2 Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00389f21",
   "metadata": {
    "id": "00389f21"
   },
   "outputs": [],
   "source": [
    "def balance_side_detection(image, results, angles):\n",
    "    \"\"\"\n",
    "    Analyzes and annotates a side-view image of a standing posture.\n",
    "\n",
    "    Args:\n",
    "    - image (numpy.ndarray): The image on which to perform the analysis and annotations.\n",
    "    - results (object): The detected pose landmarks from a pose estimation model.\n",
    "    - angles (tuple): Tuple containing angles, where angles[0] is the neck angle and angles[1] is another angle, such as the knee angle.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The annotated image with neck angle and posture information.\n",
    "    \"\"\"\n",
    "    if results.pose_landmarks:\n",
    "        landmark_idx = [7, 11, 23, 25, 27]\n",
    "        # Draw the lines\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[7], landmarks_to_draw[11])\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[11], landmarks_to_draw[23], color=(0, 0, 255))\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[23], landmarks_to_draw[25])\n",
    "        draw_colored_connection(image, results, landmarks_to_draw[25], landmarks_to_draw[27], color=(0, 0, 255))\n",
    "\n",
    "        # Draw a vertical line across the left hip\n",
    "        left_hip_landmark = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP.value]\n",
    "        left_hip_x = int(left_hip_landmark.x * image.shape[1])\n",
    "        cv2.line(image, (left_hip_x, 0), (left_hip_x, image.shape[0]), (255, 255, 0), thickness=1)\n",
    "\n",
    "        draw_landmarks(image, results, landmark_idx) # Draw circles for specified landmarks\n",
    "\n",
    "        joint_landmarks = [landmarks_to_draw[11].value, landmarks_to_draw[25].value] # Display angle information using a loop\n",
    "        draw_labeled_box(image, results, joint_landmarks, angles) # Call the draw_labeled_box function\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af504e45",
   "metadata": {
    "id": "af504e45"
   },
   "source": [
    "# 4. Run Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc67bfa1",
   "metadata": {
    "id": "dc67bfa1"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e931f69b-75f2-4a92-8ade-e9f6a2fb72c8",
   "metadata": {
    "id": "e931f69b-75f2-4a92-8ade-e9f6a2fb72c8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
